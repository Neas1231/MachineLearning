{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6_bU-RHicsb"
   },
   "source": [
    "#Линейные модели\n",
    "\n",
    "* Постановка задачи регрессии \n",
    "* Линейные модели\n",
    "* Метод наименьших квадратов\n",
    "* Метод максимального правдоподобия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLvuNeIajzJl"
   },
   "source": [
    "## Постановка задачи регрессии\n",
    "\n",
    "В математической статистике рассматриваются различные задачи - от оценивания некоторого параметра распределения и проверки гипотез до предсказания результатов будущего измерения. Для решения задач предсказания используется регрессионный анализ.\n",
    "\n",
    "Регрессионный анализ - класс статистических методов исследования влияния независимых переменных (регрессоры, предикторы) на зависимую переменную (критериальные).\n",
    "\n",
    "Регрессия занимается:\n",
    "*   Предсказанием значения зависимой переменной с помощью независимых\n",
    "*   Определение вклада отдельных независимых переменных в вариацию\n",
    "\n",
    "Пусть задан набор случайных величин $Y, X_1, ..., X_n$.  Если для каждого набора значений определено условное математическое ожидание $y(x_1, ..., x_n) = E(Y|X_1 = x_1, ..., X_n = x_n)$, то функция $y$ - регрессия величины $Y$ по величинам $X_1, ..., X_n$, а её график - линия регрессии. \n",
    "\n",
    "Существуют различные методы регрессионного анализа. Вот некоторые из них:\n",
    "* Линейная регрессия \n",
    "* Гауссова регрессия \n",
    "* Полиномиальная регрессия \n",
    "* Гребневая регрессия\n",
    "* Регрессия лассо\n",
    "* Логистическия регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_tafv2XhPib"
   },
   "source": [
    "## Линейные модели\n",
    "\n",
    "Линейная регрессия - регрессионная модель зависимости одной переменной от набора других переменных с линейной функцией зависимости.\n",
    "\n",
    "Линейная регрессионная модель - $y = f(x, b) + \\varepsilon$, где $f(x, b)$ - некоторая линейная функция $f(x, b) = b_0 + b_1x_1 + ... + b_nx_n$, а $\\varepsilon$ - случайная ошибка, математическое ожидание которой равно $ E(\\varepsilon) = 0$, а дисперсия случайных ошибок одинакова и конечна. \n",
    "\n",
    "Коэффициенты $b_i$ - коэффициенты регрессии, $x_i$ - регрессоры, $n$ - количество факторов модели.\n",
    "\n",
    "$b_i = \\frac{\\partial f}{\\partial x_i} = const$\n",
    "\n",
    "Часто параметр $b_0$ называют биас(bias, в переводе смещение или предвзятость) или константой. Удобно считать, что это коэффициент при факторе $x_0 = 1$. Тогда $f(x, b) = b_0x_0 + b_1x_1 + ... + b_nx_n = \\sum\\limits_{i = 0}^{n}b_ix_i = x^{\\top}b$.\n",
    "\n",
    "Когда фактор единственный (без учета константы), то это парная (простейшая) регрессия. В ином случае говорят о многофакторной регрессии. \n",
    "\n",
    "Рассмотрим её представление. \n",
    "\n",
    "Пусть дана выборка объема $n$ наблюдений величин $X, Y$. Номер измерения обозначим за $t$, тогда $y_t - $ значение величины $Y$ в выбранном измерении, а $x_t^{\\top} = [x_{t1}, x_{t2}, ..., x_{tk}]$ - вектор регрессоров в выбранном наблюдении. Тогда в каждом наблюдении $y_t = x_t^\\top b + \\varepsilon_t = b_1x_{t1} + b_2x_{t2} + ... +  b_kx_{tk} + \\varepsilon_t; E(\\varepsilon_t) = 0$.\n",
    "\n",
    "Если вводить обозначения $y^\\top = [y_1, y_2, ..., y_n], x = \\begin{bmatrix} x_{11} & x_{12} & ... & x_{1k} \\\\ x_{21} & x_{22} & ... & x_{2k} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n1} & x_{n2} & ... & x_{nk} \\\\\\end{bmatrix}, \\varepsilon^\\top = [\\varepsilon_1, ..., \\varepsilon_n]$, то можно записать в виде: $ y = xb + \\varepsilon$.\n",
    "\n",
    "В линейной регрессии предполагается:\n",
    "*   $D(\\varepsilon_t) = \\sigma^2 = const$\n",
    "*   $\\forall i,j, i\\neq j \\rightarrow cov(\\varepsilon_i, \\varepsilon_j) = 0$\n",
    "\n",
    "Основные методы:\n",
    "*   Метод наименьших квадратов\n",
    "*   Метод инструментальных переменных\n",
    "*   Метод максимального правдоподобия\n",
    "*   Метод моментов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a835-4xahaTv"
   },
   "source": [
    "## Метод наименьших квадратов\n",
    "\n",
    "Метод наименьших квадратов (МНК) - математический метод, основанный на минимазации суммы квадратов отклонений некоторых функций от искомых переменных. \n",
    "\n",
    "Пусть $x$ - набор неизвестных параметров, $f_i(x)$ - набор функций от этого набора переменных. Задача состоит в подборе таких параметров $x$, чтобы значение функций было как можно ближе к значениям переменных $y_i$. Задачу можно записать как нахождение решения системы уравнений $f_i(x) = y_i$. \n",
    "\n",
    "$\\sum\\limits_{i}e_i^2 = \\sum\\limits_{i}(f_i(x) - y_i)^2 \\rightarrow \\min\\limits_{x}$.\n",
    "\n",
    "Данная система уравнений не всегда имеет точное решение, в таком случае под решением понимается некоторый вектор $x$, при котором минимизируется расстояние между векторами $y$ и $f(x)$.\n",
    "\n",
    "Рассмотрим МНК в регрессионном анализе. \n",
    "\n",
    "Имеется $n$ значений величины $Y$ и соответвующих ей переменных $x$. Задача состоит в поиске взаимосвязи между переменными $x$ и $y$. На практике ставится задача апросксимации переменной $y$ некоторой функцией $f(x, b)$. \n",
    "\n",
    "$y_t = f(x_t, b) + \\varepsilon_t$\n",
    "\n",
    "$RSS(b) = \\sum\\limits_{t = 1}^{n}(y_t - f(x_t, b))^2 = e^\\top e $, где $RSS$ - сумма квадратов разностей(residual sum of squares).\n",
    "\n",
    "Надо найти $b_{OLS} = arg\\min\\limits_{b}RSS(b)$, где $OLS$ - обычный метод наименьших квадратов(ordinary least squares).\n",
    "\n",
    "Часто можно получить аналитическое решение. Для этого надо найти стационарные точки $RSS(b)$. Для этого надо решить уравнение $\\sum\\limits_{t = 1}^{n}(y_t - f(x_t, b))\\frac{\\partial f(x_t, b)}{\\partial b} = 0$.\n",
    "\n",
    "Рассмотрим МНК в линейной регрессии. \n",
    "\n",
    "$y_t = x_t^\\top b + \\varepsilon_t$\n",
    "\n",
    "$y = Xb + \\varepsilon$, тогда вектор оценок объясняемой переменной $\\hat y = Xb$. А вектор остатков регрессии $e = y - \\hat y = y - Xb$. Тогда $RSS = e^\\top e = (y - Xb)^\\top(y - Xb)$. Продифференцируем по переменной $b$.\n",
    "\n",
    "$(X^\\top X)b = X^\\top y$\n",
    "\n",
    "Решая данную систему уравнений приходим к формуле для МНК-оценок для линейной модели:\n",
    "\n",
    "$b_{OLS} = (X^\\top X)^{-1}X^\\top y = (\\frac{1}{n}X^\\top X)^{-1}\\frac{X^\\top y}{n} = (D(X))^{-1}\\frac{X^\\top y}{n}$\n",
    "\n",
    "Данные оценки обладают наименьшей дисперсией среди линейных несмещенных оценок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUKDIZdtv1Yn"
   },
   "source": [
    "## Метод максимального правдоподобия\n",
    "\n",
    "Пусть дана выборка объема $n$ наблюдения величины $X$. Введем случайную величину $f_{\\theta}(x_1, x_2, ..., x_n) = p_{\\theta}(x_1)p_{\\theta}(x_2)...p_{\\theta}(x_n)$, где $p_{\\theta}(x_i)$ - вероятность получения результата $x_i$ при параметрах $\\theta$ (можно записать $p(x_i|\\theta)$). Эта функция называется функцией правдоподобия. При рассмотрении выборки подразумевается, что различные измерения независимы, что в общем случае не является верным. В общем случае функция правдоподобия - совместное распределение выборки из параметрического распределения.  \n",
    "\n",
    "Введем величину $L_{\\theta}(x_1, ..., x_n) = ln(f_{\\theta}(x_1, x_2, ..., x_n))$. Такая функция называется логарифмической функцией правдоподобия. \n",
    "\n",
    "Оценка $\\hat\\theta(x_1, x_2, ..., x_n) = \\arg \\max\\limits_{\\theta\\in\\Theta} f_{\\theta}(x_1, x_2, ..., x_n)$ называется оценкой максимального правдоподобия. \n",
    "\n",
    "Рассмотрим теперь метод максимального правдоподобия в линейной регрессии. Модель выглядит так же: \n",
    "\n",
    "$y = xb + \\varepsilon$\n",
    "\n",
    "Предположим, что $\\varepsilon_i \\sim N(0, \\sigma^2)$. Теперь можем записать модель в новом виде :\n",
    "\n",
    "$p(y_i|x, b) = \\sum\\limits_{j = 1}^{m}b_jx_{i j} + N(0, \\sigma^2) = N(\\sum\\limits_{j = 1}^{m}b_jx_{i j}, \\sigma^2)$\n",
    "\n",
    "Рассмотрим логарифмическую функцию правдоподобия:\n",
    "\n",
    "$log(p(y|x, b)) = \\sum\\limits_{i = 1}^{n} log (N(\\sum\\limits_{j = 1}^{m}b_jx_{i j}, \\sigma^2))$\n",
    "\n",
    "Нормальное распределение имеет вид :\n",
    "\n",
    "$f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}exp(-\\frac{(x - \\mu)^2}{2\\sigma^2})$\n",
    "\n",
    "Тогда можно записать:\n",
    "\n",
    "$L_b = -\\frac{n}{2}log2\\pi\\sigma^2 - \\frac{1}{2\\sigma^2}\\sum\\limits_{i = 1}^{n}(y_i - x^\\top b)^2$\n",
    "\n",
    "Осталось найти максимум данной функции и аргумент при котором он достигается. Аргументы при которых функция правдоподобия и логарифмическая функция правдоподобия достигают максимума равны. \n",
    "\n",
    "$\\hat b = arg max_b L_b$, так как первое слагаемое логарифмической функции правдоподобия не зависит от параметра $b$, то можно записать :\n",
    "\n",
    "$\\hat b = arg max_b - \\frac{1}{2\\sigma^2}\\sum\\limits_{i = 1}^{n}(y_i - x^\\top b)^2$\n",
    "\n",
    "Можно заметить, что максимум функции правдоподобия будет достигатся при минимуме среднеквадратичной ошибки.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Линейные модели",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
