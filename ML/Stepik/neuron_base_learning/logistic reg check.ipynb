{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-17T12:26:10.678536100Z",
     "start_time": "2024-01-17T12:26:10.630482700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     Age  Gender Polyuria Polydipsia sudden weight loss weakness Polyphagia  \\\n0     40    Male       No        Yes                 No      Yes         No   \n1     58    Male       No         No                 No      Yes         No   \n2     41    Male      Yes         No                 No      Yes        Yes   \n3     45    Male       No         No                Yes      Yes        Yes   \n4     60    Male      Yes        Yes                Yes      Yes        Yes   \n..   ...     ...      ...        ...                ...      ...        ...   \n515   39  Female      Yes        Yes                Yes       No        Yes   \n516   48  Female      Yes        Yes                Yes      Yes        Yes   \n517   58  Female      Yes        Yes                Yes      Yes        Yes   \n518   32  Female       No         No                 No      Yes         No   \n519   42    Male       No         No                 No       No         No   \n\n    Genital thrush visual blurring Itching Irritability delayed healing  \\\n0               No              No     Yes           No             Yes   \n1               No             Yes      No           No              No   \n2               No              No     Yes           No             Yes   \n3              Yes              No     Yes           No             Yes   \n4               No             Yes     Yes          Yes             Yes   \n..             ...             ...     ...          ...             ...   \n515             No              No     Yes           No             Yes   \n516             No              No     Yes          Yes             Yes   \n517             No             Yes      No           No              No   \n518             No             Yes     Yes           No             Yes   \n519             No              No      No           No              No   \n\n    partial paresis muscle stiffness Alopecia Obesity     class  \n0                No              Yes      Yes     Yes  Positive  \n1               Yes               No      Yes      No  Positive  \n2                No              Yes      Yes      No  Positive  \n3                No               No       No      No  Positive  \n4               Yes              Yes      Yes     Yes  Positive  \n..              ...              ...      ...     ...       ...  \n515             Yes               No       No      No  Positive  \n516             Yes               No       No      No  Positive  \n517             Yes              Yes       No     Yes  Positive  \n518              No               No      Yes      No  Negative  \n519              No               No       No      No  Negative  \n\n[520 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Gender</th>\n      <th>Polyuria</th>\n      <th>Polydipsia</th>\n      <th>sudden weight loss</th>\n      <th>weakness</th>\n      <th>Polyphagia</th>\n      <th>Genital thrush</th>\n      <th>visual blurring</th>\n      <th>Itching</th>\n      <th>Irritability</th>\n      <th>delayed healing</th>\n      <th>partial paresis</th>\n      <th>muscle stiffness</th>\n      <th>Alopecia</th>\n      <th>Obesity</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40</td>\n      <td>Male</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>58</td>\n      <td>Male</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41</td>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>45</td>\n      <td>Male</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>60</td>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>515</th>\n      <td>39</td>\n      <td>Female</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>48</td>\n      <td>Female</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>517</th>\n      <td>58</td>\n      <td>Female</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>518</th>\n      <td>32</td>\n      <td>Female</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>42</td>\n      <td>Male</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>520 rows Ã— 17 columns</p>\n</div>"
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "df = pd.read_csv(\"./datasets/diabetes_risk_prediction_dataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "data": {
      "text/plain": "     Age  Gender  Polyuria  Polydipsia  sudden weight loss  weakness  \\\n0     40     1.0       0.0         1.0                 0.0       1.0   \n1     58     1.0       0.0         0.0                 0.0       1.0   \n2     41     1.0       1.0         0.0                 0.0       1.0   \n3     45     1.0       0.0         0.0                 1.0       1.0   \n4     60     1.0       1.0         1.0                 1.0       1.0   \n..   ...     ...       ...         ...                 ...       ...   \n515   39     0.0       1.0         1.0                 1.0       0.0   \n516   48     0.0       1.0         1.0                 1.0       1.0   \n517   58     0.0       1.0         1.0                 1.0       1.0   \n518   32     0.0       0.0         0.0                 0.0       1.0   \n519   42     1.0       0.0         0.0                 0.0       0.0   \n\n     Polyphagia  Genital thrush  visual blurring  Itching  Irritability  \\\n0           0.0             0.0              0.0      1.0           0.0   \n1           0.0             0.0              1.0      0.0           0.0   \n2           1.0             0.0              0.0      1.0           0.0   \n3           1.0             1.0              0.0      1.0           0.0   \n4           1.0             0.0              1.0      1.0           1.0   \n..          ...             ...              ...      ...           ...   \n515         1.0             0.0              0.0      1.0           0.0   \n516         1.0             0.0              0.0      1.0           1.0   \n517         1.0             0.0              1.0      0.0           0.0   \n518         0.0             0.0              1.0      1.0           0.0   \n519         0.0             0.0              0.0      0.0           0.0   \n\n     delayed healing  partial paresis  muscle stiffness  Alopecia  Obesity  \\\n0                1.0              0.0               1.0       1.0      1.0   \n1                0.0              1.0               0.0       1.0      0.0   \n2                1.0              0.0               1.0       1.0      0.0   \n3                1.0              0.0               0.0       0.0      0.0   \n4                1.0              1.0               1.0       1.0      1.0   \n..               ...              ...               ...       ...      ...   \n515              1.0              1.0               0.0       0.0      0.0   \n516              1.0              1.0               0.0       0.0      0.0   \n517              0.0              1.0               1.0       0.0      1.0   \n518              1.0              0.0               0.0       1.0      0.0   \n519              0.0              0.0               0.0       0.0      0.0   \n\n     class  \n0      1.0  \n1      1.0  \n2      1.0  \n3      1.0  \n4      1.0  \n..     ...  \n515    1.0  \n516    1.0  \n517    1.0  \n518    0.0  \n519    0.0  \n\n[520 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Gender</th>\n      <th>Polyuria</th>\n      <th>Polydipsia</th>\n      <th>sudden weight loss</th>\n      <th>weakness</th>\n      <th>Polyphagia</th>\n      <th>Genital thrush</th>\n      <th>visual blurring</th>\n      <th>Itching</th>\n      <th>Irritability</th>\n      <th>delayed healing</th>\n      <th>partial paresis</th>\n      <th>muscle stiffness</th>\n      <th>Alopecia</th>\n      <th>Obesity</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>58</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>45</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>60</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>515</th>\n      <td>39</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>48</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>517</th>\n      <td>58</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>518</th>\n      <td>32</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>42</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>520 rows Ã— 17 columns</p>\n</div>"
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "oe = OrdinalEncoder()\n",
    "df[df.select_dtypes(object).columns] = oe.fit_transform(df.select_dtypes(object))\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T12:26:10.952684500Z",
     "start_time": "2024-01-17T12:26:10.859973400Z"
    }
   },
   "id": "30ad71b4eb041e30"
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = torch.tensor(df.drop(columns=['class']).values).to(torch.float32)\n",
    "y = torch.tensor(df['class'].values).to(torch.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T12:26:11.056851700Z",
     "start_time": "2024-01-17T12:26:11.028758900Z"
    }
   },
   "id": "f697ee0cb1ea11c9"
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Logistic,self).__init__()\n",
    "        self.fc1 = nn.Linear(16, 16)\n",
    "        self.tahn = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(16,1)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.tahn(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigm(x)\n",
    "        return x\n",
    "\n",
    "model = Logistic()\n",
    "optim = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_func = nn.BCELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T12:26:18.988618400Z",
     "start_time": "2024-01-17T12:26:18.979906200Z"
    }
   },
   "id": "91136c7faa1240bd"
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.5756],\n        [0.5683],\n        [0.5734],\n        [0.5732],\n        [0.5720],\n        [0.5730],\n        [0.5719],\n        [0.5746],\n        [0.5719],\n        [0.5734],\n        [0.5720],\n        [0.5757],\n        [0.5751],\n        [0.5734],\n        [0.5733],\n        [0.5717],\n        [0.5790],\n        [0.5736],\n        [0.5722],\n        [0.5771],\n        [0.5745],\n        [0.5722],\n        [0.5740],\n        [0.5711],\n        [0.5735],\n        [0.5738],\n        [0.5735],\n        [0.5723],\n        [0.5731],\n        [0.5736],\n        [0.5719],\n        [0.5722],\n        [0.5738],\n        [0.5749],\n        [0.5720],\n        [0.5734],\n        [0.5743],\n        [0.5720],\n        [0.5732],\n        [0.5722],\n        [0.5683],\n        [0.5718],\n        [0.5733],\n        [0.5735],\n        [0.5736],\n        [0.5720],\n        [0.5729],\n        [0.5744],\n        [0.5732],\n        [0.5711],\n        [0.5739],\n        [0.5731],\n        [0.5720],\n        [0.5754],\n        [0.5724],\n        [0.5728],\n        [0.5745],\n        [0.5735],\n        [0.5732],\n        [0.5720],\n        [0.5747],\n        [0.5705],\n        [0.5733],\n        [0.5726],\n        [0.5753],\n        [0.5724],\n        [0.5716],\n        [0.5728],\n        [0.5724],\n        [0.5774],\n        [0.5723],\n        [0.5726],\n        [0.5741],\n        [0.5723],\n        [0.5756],\n        [0.5710],\n        [0.5738],\n        [0.5736],\n        [0.5734],\n        [0.5705],\n        [0.5728],\n        [0.5739],\n        [0.5729],\n        [0.5737],\n        [0.5732],\n        [0.5730],\n        [0.5732],\n        [0.5742],\n        [0.5731],\n        [0.5711],\n        [0.5723],\n        [0.5723],\n        [0.5744],\n        [0.5742],\n        [0.5732],\n        [0.5746],\n        [0.5745],\n        [0.5722],\n        [0.5711],\n        [0.5745],\n        [0.5741],\n        [0.5693],\n        [0.5724],\n        [0.5757],\n        [0.5738],\n        [0.5731],\n        [0.5719],\n        [0.5724],\n        [0.5773],\n        [0.5723],\n        [0.5786],\n        [0.5737],\n        [0.5742],\n        [0.5787],\n        [0.5741],\n        [0.5732],\n        [0.5711],\n        [0.5703],\n        [0.5747],\n        [0.5732],\n        [0.5787],\n        [0.5716],\n        [0.5721],\n        [0.5724],\n        [0.5745],\n        [0.5710],\n        [0.5731],\n        [0.5725],\n        [0.5751],\n        [0.5711],\n        [0.5768],\n        [0.5714],\n        [0.5787],\n        [0.5725],\n        [0.5751],\n        [0.5731],\n        [0.5716],\n        [0.5720],\n        [0.5751],\n        [0.5722],\n        [0.5729],\n        [0.5722],\n        [0.5756],\n        [0.5731],\n        [0.5773],\n        [0.5723],\n        [0.5722],\n        [0.5772],\n        [0.5723],\n        [0.5728],\n        [0.5737],\n        [0.5714],\n        [0.5716],\n        [0.5725],\n        [0.5703],\n        [0.5755],\n        [0.5711],\n        [0.5750],\n        [0.5731],\n        [0.5719],\n        [0.5722],\n        [0.5715],\n        [0.5725],\n        [0.5717],\n        [0.5723],\n        [0.5724],\n        [0.5721],\n        [0.5723],\n        [0.5775],\n        [0.5734],\n        [0.5722],\n        [0.5713],\n        [0.5705],\n        [0.5724],\n        [0.5745],\n        [0.5746],\n        [0.5719],\n        [0.5738],\n        [0.5728],\n        [0.5720],\n        [0.5714],\n        [0.5732],\n        [0.5751],\n        [0.5744],\n        [0.5751],\n        [0.5711],\n        [0.5728],\n        [0.5757],\n        [0.5703],\n        [0.5742],\n        [0.5757],\n        [0.5745],\n        [0.5730],\n        [0.5741],\n        [0.5731],\n        [0.5734],\n        [0.5721],\n        [0.5728],\n        [0.5720],\n        [0.5720],\n        [0.5735],\n        [0.5761],\n        [0.5692],\n        [0.5735],\n        [0.5722],\n        [0.5720],\n        [0.5711],\n        [0.5746],\n        [0.5728],\n        [0.5734],\n        [0.5771],\n        [0.5724],\n        [0.5722],\n        [0.5774],\n        [0.5745],\n        [0.5756],\n        [0.5772],\n        [0.5790],\n        [0.5724],\n        [0.5729],\n        [0.5697],\n        [0.5736],\n        [0.5711],\n        [0.5745],\n        [0.5764],\n        [0.5715],\n        [0.5728],\n        [0.5425],\n        [0.5710],\n        [0.5746],\n        [0.5751],\n        [0.5709],\n        [0.5724],\n        [0.5716],\n        [0.5725],\n        [0.5717],\n        [0.5734],\n        [0.5790],\n        [0.5777],\n        [0.5736],\n        [0.5734],\n        [0.5733],\n        [0.5720],\n        [0.5724],\n        [0.5724],\n        [0.5746],\n        [0.5727],\n        [0.5730],\n        [0.5732],\n        [0.5743],\n        [0.5711],\n        [0.5725],\n        [0.5749],\n        [0.5731],\n        [0.5725],\n        [0.5744],\n        [0.5724],\n        [0.5716],\n        [0.5738],\n        [0.5731],\n        [0.5744],\n        [0.5753],\n        [0.5738],\n        [0.5746],\n        [0.5746],\n        [0.5711],\n        [0.5731],\n        [0.5715],\n        [0.5733],\n        [0.5736],\n        [0.5740],\n        [0.5738],\n        [0.5716],\n        [0.5774],\n        [0.5729],\n        [0.5757],\n        [0.5714],\n        [0.5719],\n        [0.5750],\n        [0.5703],\n        [0.5768],\n        [0.5724],\n        [0.5723],\n        [0.5713],\n        [0.5736],\n        [0.5746],\n        [0.5714],\n        [0.5724],\n        [0.5729],\n        [0.5775],\n        [0.5746],\n        [0.5739],\n        [0.5704],\n        [0.5734],\n        [0.5724],\n        [0.5744],\n        [0.5725],\n        [0.5721],\n        [0.5731],\n        [0.5738],\n        [0.5738],\n        [0.5736],\n        [0.5745],\n        [0.5711],\n        [0.5723],\n        [0.5724],\n        [0.5732],\n        [0.5746],\n        [0.5731],\n        [0.5746],\n        [0.5722],\n        [0.5735],\n        [0.5720],\n        [0.5722],\n        [0.5722],\n        [0.5746],\n        [0.5725],\n        [0.5731],\n        [0.5710],\n        [0.5719],\n        [0.5717],\n        [0.5771],\n        [0.5697],\n        [0.5725],\n        [0.5761],\n        [0.5728],\n        [0.5719],\n        [0.5723],\n        [0.5711],\n        [0.5736],\n        [0.5726],\n        [0.5732],\n        [0.5744],\n        [0.5728],\n        [0.5723],\n        [0.5718],\n        [0.5763],\n        [0.5731],\n        [0.5731],\n        [0.5732],\n        [0.5733],\n        [0.5722],\n        [0.5733],\n        [0.5737],\n        [0.5743],\n        [0.5771],\n        [0.5771],\n        [0.5725],\n        [0.5731],\n        [0.5741],\n        [0.5719],\n        [0.5790],\n        [0.5730],\n        [0.5725],\n        [0.5712],\n        [0.5725],\n        [0.5719],\n        [0.5716],\n        [0.5734],\n        [0.5724],\n        [0.5724],\n        [0.5729],\n        [0.5697],\n        [0.5721],\n        [0.5709],\n        [0.5725],\n        [0.5726],\n        [0.5725],\n        [0.5786],\n        [0.5738],\n        [0.5750],\n        [0.5736],\n        [0.5726],\n        [0.5725],\n        [0.5728],\n        [0.5732],\n        [0.5720],\n        [0.5773],\n        [0.5718],\n        [0.5703],\n        [0.5721],\n        [0.5722],\n        [0.5700],\n        [0.5745],\n        [0.5774],\n        [0.5734],\n        [0.5734],\n        [0.5731],\n        [0.5711],\n        [0.5750],\n        [0.5728],\n        [0.5717],\n        [0.5751],\n        [0.5728],\n        [0.5751],\n        [0.5723],\n        [0.5747],\n        [0.5717],\n        [0.5770],\n        [0.5754],\n        [0.5774],\n        [0.5737],\n        [0.5746],\n        [0.5728],\n        [0.5731],\n        [0.5787],\n        [0.5726],\n        [0.5723],\n        [0.5720],\n        [0.5793],\n        [0.5742],\n        [0.5754],\n        [0.5723],\n        [0.5787],\n        [0.5734],\n        [0.5717]], grad_fn=<SigmoidBackward0>)"
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T12:26:19.576315700Z",
     "start_time": "2024-01-17T12:26:19.523979500Z"
    }
   },
   "id": "48b0e02bd35b69ce"
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.85      0.88        33\n",
      "         1.0       0.93      0.96      0.94        71\n",
      "\n",
      "    accuracy                           0.92       104\n",
      "   macro avg       0.92      0.90      0.91       104\n",
      "weighted avg       0.92      0.92      0.92       104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "sk_model = LogisticRegression(max_iter=1000)\n",
    "sk_model.fit(X_train, y_train)\n",
    "y_preds = sk_model.predict(X_test)\n",
    "print(classification_report(y_test, y_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T12:26:19.734734900Z",
     "start_time": "2024-01-17T12:26:19.693183Z"
    }
   },
   "id": "f01180cd9296faec"
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "for epoch in range(1000):\n",
    "    optim.zero_grad()\n",
    "    #print(loss_func(y_train,model(X_train).reshape(-1)))\n",
    "    loss = loss_func(model(X_train).reshape(-1),y_train)\n",
    "    loss.backward()\n",
    "    optim.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T12:26:22.073974900Z",
     "start_time": "2024-01-17T12:26:20.551634500Z"
    }
   },
   "id": "f682d92b92b74ebf"
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.9987],\n        [0.9997],\n        [0.2679],\n        [0.2174],\n        [0.0185],\n        [0.1083],\n        [0.0185],\n        [0.9996],\n        [0.1430],\n        [0.2679],\n        [0.0901],\n        [0.9896],\n        [0.0375],\n        [0.0570],\n        [0.7740],\n        [0.0391],\n        [0.9997],\n        [0.6873],\n        [0.9989],\n        [0.9996],\n        [0.9887],\n        [0.9969],\n        [0.9989],\n        [0.0192],\n        [0.9989],\n        [0.0187],\n        [0.9996],\n        [0.9453],\n        [0.9995],\n        [0.9967],\n        [0.1430],\n        [0.7850],\n        [0.9989],\n        [0.9958],\n        [0.0233],\n        [0.9952],\n        [0.1927],\n        [0.0185],\n        [0.9976],\n        [0.9989],\n        [0.9997],\n        [0.9801],\n        [0.9716],\n        [0.0106],\n        [0.9171],\n        [0.9656],\n        [0.2560],\n        [0.9944],\n        [0.9219],\n        [0.0192],\n        [0.9988],\n        [0.0105],\n        [0.0590],\n        [0.9990],\n        [0.0748],\n        [0.1016],\n        [0.9478],\n        [0.0106],\n        [0.9219],\n        [0.0185],\n        [0.6764],\n        [0.9998],\n        [0.7740],\n        [0.1980],\n        [0.9936],\n        [0.9988],\n        [0.1438],\n        [0.2579],\n        [0.0748],\n        [0.9914],\n        [0.9956],\n        [0.5208],\n        [0.0294],\n        [0.0129],\n        [0.9987],\n        [0.0308],\n        [0.0187],\n        [0.6873],\n        [0.9979],\n        [0.9993],\n        [0.1016],\n        [0.9954],\n        [0.9877],\n        [0.9991],\n        [0.2174],\n        [0.9992],\n        [0.9968],\n        [0.9991],\n        [0.9962],\n        [0.0192],\n        [0.0129],\n        [0.9977],\n        [0.9106],\n        [0.9943],\n        [0.2174],\n        [0.0306],\n        [0.9993],\n        [0.0091],\n        [0.0192],\n        [0.9980],\n        [0.0294],\n        [0.9914],\n        [0.0577],\n        [0.1354],\n        [0.9919],\n        [0.0172],\n        [0.1430],\n        [0.0827],\n        [0.8748],\n        [0.9288],\n        [0.9999],\n        [0.9991],\n        [0.9943],\n        [0.9996],\n        [0.9778],\n        [0.2980],\n        [0.0192],\n        [0.0201],\n        [0.9979],\n        [0.9642],\n        [0.9996],\n        [0.8437],\n        [0.9989],\n        [0.9988],\n        [0.9973],\n        [0.0308],\n        [0.0320],\n        [0.9949],\n        [0.9994],\n        [0.0192],\n        [0.9864],\n        [0.5662],\n        [0.9996],\n        [0.9982],\n        [0.0375],\n        [0.9507],\n        [0.8437],\n        [0.0127],\n        [0.0375],\n        [0.0153],\n        [0.9671],\n        [0.6538],\n        [0.3915],\n        [0.0395],\n        [0.9979],\n        [0.9288],\n        [0.9997],\n        [0.9998],\n        [0.0129],\n        [0.0518],\n        [0.9995],\n        [0.5662],\n        [0.1438],\n        [0.9949],\n        [0.0201],\n        [0.9940],\n        [0.0192],\n        [0.9158],\n        [0.0395],\n        [0.9947],\n        [0.9969],\n        [0.9669],\n        [0.0343],\n        [0.7718],\n        [0.8932],\n        [0.9988],\n        [0.3955],\n        [0.9977],\n        [0.9975],\n        [0.8118],\n        [0.0584],\n        [0.5680],\n        [0.9993],\n        [0.0748],\n        [0.8316],\n        [0.9998],\n        [0.0185],\n        [0.0187],\n        [0.4173],\n        [0.0594],\n        [0.5662],\n        [0.2980],\n        [0.0375],\n        [0.0115],\n        [0.9932],\n        [0.0192],\n        [0.0180],\n        [0.3226],\n        [0.0201],\n        [0.9703],\n        [0.1354],\n        [0.9993],\n        [0.9981],\n        [0.0294],\n        [0.0395],\n        [0.9952],\n        [0.9989],\n        [0.0518],\n        [0.9596],\n        [0.0127],\n        [0.9998],\n        [0.9979],\n        [0.7825],\n        [0.9989],\n        [0.9997],\n        [0.0185],\n        [0.0192],\n        [0.0147],\n        [0.0514],\n        [0.0570],\n        [0.9996],\n        [0.3679],\n        [0.9969],\n        [0.9992],\n        [0.1189],\n        [0.9998],\n        [0.9733],\n        [0.9997],\n        [0.0446],\n        [0.9671],\n        [0.0384],\n        [0.7094],\n        [0.0192],\n        [0.9478],\n        [0.9606],\n        [0.9669],\n        [0.0518],\n        [0.9137],\n        [0.0308],\n        [0.9995],\n        [0.0375],\n        [0.9957],\n        [0.0748],\n        [0.7394],\n        [0.0099],\n        [0.8958],\n        [0.0570],\n        [0.9997],\n        [0.9970],\n        [0.9967],\n        [0.9457],\n        [0.9756],\n        [0.9596],\n        [0.9988],\n        [0.0827],\n        [0.0306],\n        [0.9979],\n        [0.9993],\n        [0.0497],\n        [0.4376],\n        [0.0192],\n        [0.0343],\n        [0.9958],\n        [0.9993],\n        [0.9949],\n        [0.0115],\n        [0.0827],\n        [0.1438],\n        [0.9989],\n        [0.9995],\n        [0.9944],\n        [0.9953],\n        [0.9979],\n        [0.0306],\n        [0.9996],\n        [0.0106],\n        [0.9962],\n        [0.2757],\n        [0.9756],\n        [0.9967],\n        [0.9989],\n        [0.0187],\n        [0.1438],\n        [0.9992],\n        [0.9988],\n        [0.1354],\n        [0.5662],\n        [0.1430],\n        [0.7433],\n        [0.0201],\n        [0.9864],\n        [0.1448],\n        [0.2791],\n        [0.6750],\n        [0.9735],\n        [0.0306],\n        [0.5662],\n        [0.1448],\n        [0.9877],\n        [0.9996],\n        [0.9976],\n        [0.7666],\n        [0.9885],\n        [0.0570],\n        [0.9948],\n        [0.0115],\n        [0.9982],\n        [0.7637],\n        [0.9995],\n        [0.9667],\n        [0.9989],\n        [0.9984],\n        [0.9980],\n        [0.0192],\n        [0.9288],\n        [0.0748],\n        [0.2174],\n        [0.0306],\n        [0.9507],\n        [0.9995],\n        [0.9989],\n        [0.0162],\n        [0.0127],\n        [0.1264],\n        [0.9989],\n        [0.0306],\n        [0.9982],\n        [0.0294],\n        [0.0308],\n        [0.9947],\n        [0.0186],\n        [0.9996],\n        [0.0384],\n        [0.9949],\n        [0.9979],\n        [0.1016],\n        [0.0185],\n        [0.8722],\n        [0.0192],\n        [0.9171],\n        [0.9895],\n        [0.9911],\n        [0.0115],\n        [0.6778],\n        [0.0129],\n        [0.0185],\n        [0.9953],\n        [0.9995],\n        [0.9995],\n        [0.2980],\n        [0.9756],\n        [0.7850],\n        [0.9716],\n        [0.9841],\n        [0.4376],\n        [0.9996],\n        [0.9996],\n        [0.9982],\n        [0.0395],\n        [0.9997],\n        [0.9947],\n        [0.9997],\n        [0.3473],\n        [0.9940],\n        [0.5699],\n        [0.9949],\n        [0.0185],\n        [0.1438],\n        [0.9952],\n        [0.1448],\n        [0.9986],\n        [0.9941],\n        [0.0384],\n        [0.0194],\n        [0.6746],\n        [0.0573],\n        [0.8781],\n        [0.9949],\n        [0.9999],\n        [0.0187],\n        [0.9158],\n        [0.9984],\n        [0.9895],\n        [0.9995],\n        [0.1016],\n        [0.9642],\n        [0.0185],\n        [0.9979],\n        [0.5130],\n        [0.0201],\n        [0.9982],\n        [0.9969],\n        [0.0205],\n        [0.9993],\n        [0.9914],\n        [0.2679],\n        [0.9952],\n        [0.9507],\n        [0.0192],\n        [0.9158],\n        [0.9935],\n        [0.7718],\n        [0.0375],\n        [0.0518],\n        [0.0375],\n        [0.9288],\n        [0.9979],\n        [0.0186],\n        [0.9945],\n        [0.9996],\n        [0.9996],\n        [0.9991],\n        [0.9998],\n        [0.1016],\n        [0.9507],\n        [0.9996],\n        [0.1980],\n        [0.0129],\n        [0.0185],\n        [0.9888],\n        [0.9991],\n        [0.9635],\n        [0.8722],\n        [0.9996],\n        [0.9952],\n        [0.8958]], grad_fn=<SigmoidBackward0>)"
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T12:26:23.414877800Z",
     "start_time": "2024-01-17T12:26:23.401249800Z"
    }
   },
   "id": "728d8ed5480cad4e"
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n        0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n        0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n        1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n        1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n        1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n        1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n        1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n        0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n        1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n        1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n        1., 1.])"
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T12:26:26.330396Z",
     "start_time": "2024-01-17T12:26:26.265027800Z"
    }
   },
   "id": "9bb746f6683f76b3"
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.9987, 0.9997, 0.2679, 0.2174, 0.0185, 0.1083, 0.0185, 0.9996, 0.1430,\n        0.2679, 0.0901, 0.9896, 0.0375, 0.0570, 0.7740, 0.0391, 0.9997, 0.6873,\n        0.9989, 0.9996, 0.9887, 0.9969, 0.9989, 0.0192, 0.9989, 0.0187, 0.9996,\n        0.9453, 0.9995, 0.9967, 0.1430, 0.7850, 0.9989, 0.9958, 0.0233, 0.9952,\n        0.1927, 0.0185, 0.9976, 0.9989, 0.9997, 0.9801, 0.9716, 0.0106, 0.9171,\n        0.9656, 0.2560, 0.9944, 0.9219, 0.0192, 0.9988, 0.0105, 0.0590, 0.9990,\n        0.0748, 0.1016, 0.9478, 0.0106, 0.9219, 0.0185, 0.6764, 0.9998, 0.7740,\n        0.1980, 0.9936, 0.9988, 0.1438, 0.2579, 0.0748, 0.9914, 0.9956, 0.5208,\n        0.0294, 0.0129, 0.9987, 0.0308, 0.0187, 0.6873, 0.9979, 0.9993, 0.1016,\n        0.9954, 0.9877, 0.9991, 0.2174, 0.9992, 0.9968, 0.9991, 0.9962, 0.0192,\n        0.0129, 0.9977, 0.9106, 0.9943, 0.2174, 0.0306, 0.9993, 0.0091, 0.0192,\n        0.9980, 0.0294, 0.9914, 0.0577, 0.1354, 0.9919, 0.0172, 0.1430, 0.0827,\n        0.8748, 0.9288, 0.9999, 0.9991, 0.9943, 0.9996, 0.9778, 0.2980, 0.0192,\n        0.0201, 0.9979, 0.9642, 0.9996, 0.8437, 0.9989, 0.9988, 0.9973, 0.0308,\n        0.0320, 0.9949, 0.9994, 0.0192, 0.9864, 0.5662, 0.9996, 0.9982, 0.0375,\n        0.9507, 0.8437, 0.0127, 0.0375, 0.0153, 0.9671, 0.6538, 0.3915, 0.0395,\n        0.9979, 0.9288, 0.9997, 0.9998, 0.0129, 0.0518, 0.9995, 0.5662, 0.1438,\n        0.9949, 0.0201, 0.9940, 0.0192, 0.9158, 0.0395, 0.9947, 0.9969, 0.9669,\n        0.0343, 0.7718, 0.8932, 0.9988, 0.3955, 0.9977, 0.9975, 0.8118, 0.0584,\n        0.5680, 0.9993, 0.0748, 0.8316, 0.9998, 0.0185, 0.0187, 0.4173, 0.0594,\n        0.5662, 0.2980, 0.0375, 0.0115, 0.9932, 0.0192, 0.0180, 0.3226, 0.0201,\n        0.9703, 0.1354, 0.9993, 0.9981, 0.0294, 0.0395, 0.9952, 0.9989, 0.0518,\n        0.9596, 0.0127, 0.9998, 0.9979, 0.7825, 0.9989, 0.9997, 0.0185, 0.0192,\n        0.0147, 0.0514, 0.0570, 0.9996, 0.3679, 0.9969, 0.9992, 0.1189, 0.9998,\n        0.9733, 0.9997, 0.0446, 0.9671, 0.0384, 0.7094, 0.0192, 0.9478, 0.9606,\n        0.9669, 0.0518, 0.9137, 0.0308, 0.9995, 0.0375, 0.9957, 0.0748, 0.7394,\n        0.0099, 0.8958, 0.0570, 0.9997, 0.9970, 0.9967, 0.9457, 0.9756, 0.9596,\n        0.9988, 0.0827, 0.0306, 0.9979, 0.9993, 0.0497, 0.4376, 0.0192, 0.0343,\n        0.9958, 0.9993, 0.9949, 0.0115, 0.0827, 0.1438, 0.9989, 0.9995, 0.9944,\n        0.9953, 0.9979, 0.0306, 0.9996, 0.0106, 0.9962, 0.2757, 0.9756, 0.9967,\n        0.9989, 0.0187, 0.1438, 0.9992, 0.9988, 0.1354, 0.5662, 0.1430, 0.7433,\n        0.0201, 0.9864, 0.1448, 0.2791, 0.6750, 0.9735, 0.0306, 0.5662, 0.1448,\n        0.9877, 0.9996, 0.9976, 0.7666, 0.9885, 0.0570, 0.9948, 0.0115, 0.9982,\n        0.7637, 0.9995, 0.9667, 0.9989, 0.9984, 0.9980, 0.0192, 0.9288, 0.0748,\n        0.2174, 0.0306, 0.9507, 0.9995, 0.9989, 0.0162, 0.0127, 0.1264, 0.9989,\n        0.0306, 0.9982, 0.0294, 0.0308, 0.9947, 0.0186, 0.9996, 0.0384, 0.9949,\n        0.9979, 0.1016, 0.0185, 0.8722, 0.0192, 0.9171, 0.9895, 0.9911, 0.0115,\n        0.6778, 0.0129, 0.0185, 0.9953, 0.9995, 0.9995, 0.2980, 0.9756, 0.7850,\n        0.9716, 0.9841, 0.4376, 0.9996, 0.9996, 0.9982, 0.0395, 0.9997, 0.9947,\n        0.9997, 0.3473, 0.9940, 0.5699, 0.9949, 0.0185, 0.1438, 0.9952, 0.1448,\n        0.9986, 0.9941, 0.0384, 0.0194, 0.6746, 0.0573, 0.8781, 0.9949, 0.9999,\n        0.0187, 0.9158, 0.9984, 0.9895, 0.9995, 0.1016, 0.9642, 0.0185, 0.9979,\n        0.5130, 0.0201, 0.9982, 0.9969, 0.0205, 0.9993, 0.9914, 0.2679, 0.9952,\n        0.9507, 0.0192, 0.9158, 0.9935, 0.7718, 0.0375, 0.0518, 0.0375, 0.9288,\n        0.9979, 0.0186, 0.9945, 0.9996, 0.9996, 0.9991, 0.9998, 0.1016, 0.9507,\n        0.9996, 0.1980, 0.0129, 0.0185, 0.9888, 0.9991, 0.9635, 0.8722, 0.9996,\n        0.9952, 0.8958], grad_fn=<ViewBackward0>)"
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train).reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T12:26:27.015213700Z",
     "start_time": "2024-01-17T12:26:26.971613800Z"
    }
   },
   "id": "f2efffd39f6408fb"
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.91      0.92        33\n",
      "         1.0       0.96      0.97      0.97        71\n",
      "\n",
      "    accuracy                           0.95       104\n",
      "   macro avg       0.95      0.94      0.94       104\n",
      "weighted avg       0.95      0.95      0.95       104\n"
     ]
    }
   ],
   "source": [
    "y_preds = model.forward(X_test)\n",
    "print(classification_report(y_test,np.round(y_preds.detach().numpy())))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T12:27:12.544735300Z",
     "start_time": "2024-01-17T12:27:12.518512200Z"
    }
   },
   "id": "fe5649ea03abffbc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
