{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b0e58a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T06:26:00.598523800Z",
     "start_time": "2024-02-02T06:25:56.795205200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "\n",
    "mnist_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=0.5, std=0.5)])\n",
    "\n",
    "data = datasets.MNIST(root='/data/MNIST', download=True, transform=mnist_transforms)\n",
    "\n",
    "mnist_dataloader = DataLoader(data, batch_size=64, shuffle=True, num_workers=0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c8a0ced",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T06:26:00.600044300Z",
     "start_time": "2024-02-02T06:26:00.592205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset MNIST\n    Number of datapoints: 60000\n    Root location: /data/MNIST\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n               Normalize(mean=0.5, std=0.5)\n           )"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([64, 1, 28, 28])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = iter(mnist_dataloader)\n",
    "X = next(test_data)\n",
    "print(len(X))\n",
    "X[0].size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T06:26:00.649092800Z",
     "start_time": "2024-02-02T06:26:00.595522900Z"
    }
   },
   "id": "abb6f915f4a0c9b7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8980,\n          -0.7020, -0.8902, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.7176,  0.1216,  0.6314,\n           0.9843, -0.0275, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.9686, -0.0431,  0.8588,  0.9843,  0.9843,\n           0.9843,  0.9373, -0.2471, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.9686,  0.6627,  0.9843,  0.9843,  0.9843,  0.9843,\n           0.9843,  0.9843,  0.3882, -0.9765, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.3176,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n           0.9843,  0.9843,  0.9843, -0.2392, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.5059,  0.9843,  0.9843,  0.9451,  0.2078, -0.7882,\n          -0.7882,  0.3490,  0.9843,  0.0039, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.9608,  0.6078,  0.6078, -0.5059, -1.0000, -1.0000,\n          -1.0000, -0.2471,  0.9843,  0.8196, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.2784,  0.9843,  0.8196, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.1373,  0.9843,  0.8196, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.0275,  0.9843,  0.8196, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000,  0.2706,  0.9843,  0.5059, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.9137, -0.8588, -1.0000,\n          -0.8039,  0.9451,  0.9843, -0.0980, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9294,\n          -0.2706,  0.3176,  0.3176,  0.3176,  0.6314,  0.8588,  0.3176,\n           0.6471,  0.9843,  0.9843, -0.6784, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.7333,  0.0667, -0.6078, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.0039,\n           0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n           0.9843,  0.9843,  0.9843,  0.7961,  0.6157,  0.6157,  0.6157,\n           0.2471,  0.6157,  0.8196,  0.9843,  0.1373, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9608,\n           0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n           0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n           0.9843,  0.9843,  0.9843,  0.9843,  0.8667, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9922,\n           0.9843,  0.9765,  0.4745,  0.8588,  0.9843,  0.9843,  0.9843,\n           0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n           0.9843,  0.9843,  0.9843,  0.9843, -0.0745, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9922,\n           0.9843,  0.9843,  0.6549,  0.9059,  0.9843,  0.9843,  0.9843,\n          -0.1686, -0.6627, -0.3569,  0.1765,  0.1765,  0.1765,  0.1765,\n           0.3882,  0.6627,  0.5216, -0.4118, -0.9451, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1608,\n           0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9373, -0.2078,\n          -0.9922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.9843, -0.9765, -0.9843, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6549,\n           0.3647,  0.9843,  0.9843,  0.9843,  0.4902, -0.2000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.9059, -0.7098, -0.7098, -0.7098, -0.9686, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T06:26:00.681092900Z",
     "start_time": "2024-02-02T06:26:00.642093Z"
    }
   },
   "id": "c276a536f7ddd95f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "743c9b52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T09:02:15.073509100Z",
     "start_time": "2024-02-01T09:02:15.071797200Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator class. Accepts a tensor of size 100 as input as outputs another\n",
    "    tensor of size 784. Objective is to generate an output tensor that is\n",
    "    indistinguishable from the real MNIST digits \n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(in_features=128, out_features=256),\n",
    "                                    nn.LeakyReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Linear(in_features=256, out_features=512),\n",
    "                                    nn.LeakyReLU())\n",
    "        self.layer3 = nn.Sequential(nn.Linear(in_features=512, out_features=1024),\n",
    "                                    nn.LeakyReLU())\n",
    "        self.output = nn.Linear(in_features=1024, out_features=28 * 28)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ccfe0ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T09:02:16.413785100Z",
     "start_time": "2024-02-01T09:02:16.408523500Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator class. Accepts a tensor of size 784 as input and outputs\n",
    "    a tensor of size 1 as  the predicted class probabilities\n",
    "    (generated or real data)\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.hidden = nn.ModuleList()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(in_features=28 * 28, out_features=1024),\n",
    "                                    nn.LeakyReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Linear(in_features=1024, out_features=512),\n",
    "                                    nn.LeakyReLU())\n",
    "        self.layer3 = nn.Sequential(nn.Linear(in_features=512, out_features=256),\n",
    "                                    nn.LeakyReLU())\n",
    "        self.output = nn.Sequential(nn.Linear(in_features=256, out_features=1),\n",
    "                                    nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "D = Discriminator().to(device)\n",
    "G = Generator().to(device)\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=0.0001)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T14:37:28.405062600Z",
     "start_time": "2024-01-31T14:37:28.302572300Z"
    }
   },
   "id": "2821a2eaaea6a13f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "for i in enumerate(mnist_dataloader):\n",
    "    print(i[1][0].size())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f71e69f7867fc8"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 100: discriminator_loss 0.171 generator_loss 1.783\n",
      "Epoch 0 Iteration 200: discriminator_loss 0.099 generator_loss 1.797\n",
      "Epoch 0 Iteration 300: discriminator_loss 0.126 generator_loss 3.859\n",
      "Epoch 0 Iteration 400: discriminator_loss 0.155 generator_loss 5.184\n",
      "Epoch 0 Iteration 500: discriminator_loss 0.229 generator_loss 4.452\n",
      "Epoch 0 Iteration 600: discriminator_loss 0.134 generator_loss 5.287\n",
      "Epoch 0 Iteration 700: discriminator_loss 0.207 generator_loss 4.386\n",
      "Epoch 0 Iteration 800: discriminator_loss 0.162 generator_loss 4.518\n",
      "Epoch 0 Iteration 900: discriminator_loss 0.127 generator_loss 5.908\n",
      "Epoch 0 Iteration 938: discriminator_loss 0.380 generator_loss 4.894\n",
      "Epoch 1 Iteration 100: discriminator_loss 0.259 generator_loss 2.886\n",
      "Epoch 1 Iteration 200: discriminator_loss 0.038 generator_loss 4.968\n",
      "Epoch 1 Iteration 300: discriminator_loss 0.031 generator_loss 4.555\n",
      "Epoch 1 Iteration 400: discriminator_loss 0.020 generator_loss 5.344\n",
      "Epoch 1 Iteration 500: discriminator_loss 0.057 generator_loss 5.162\n",
      "Epoch 1 Iteration 600: discriminator_loss 0.074 generator_loss 7.039\n",
      "Epoch 1 Iteration 700: discriminator_loss 0.185 generator_loss 7.910\n",
      "Epoch 1 Iteration 800: discriminator_loss 0.254 generator_loss 2.646\n",
      "Epoch 1 Iteration 900: discriminator_loss 0.097 generator_loss 3.395\n",
      "Epoch 1 Iteration 938: discriminator_loss 0.023 generator_loss 5.208\n",
      "Epoch 2 Iteration 100: discriminator_loss 0.024 generator_loss 4.627\n",
      "Epoch 2 Iteration 200: discriminator_loss 0.046 generator_loss 4.266\n",
      "Epoch 2 Iteration 300: discriminator_loss 0.032 generator_loss 5.481\n",
      "Epoch 2 Iteration 400: discriminator_loss 0.089 generator_loss 10.471\n",
      "Epoch 2 Iteration 500: discriminator_loss 0.017 generator_loss 6.597\n",
      "Epoch 2 Iteration 600: discriminator_loss 0.037 generator_loss 6.667\n",
      "Epoch 2 Iteration 700: discriminator_loss 0.055 generator_loss 9.972\n",
      "Epoch 2 Iteration 800: discriminator_loss 0.075 generator_loss 7.945\n",
      "Epoch 2 Iteration 900: discriminator_loss 0.024 generator_loss 16.486\n",
      "Epoch 2 Iteration 938: discriminator_loss 0.050 generator_loss 4.262\n",
      "Epoch 3 Iteration 100: discriminator_loss 0.039 generator_loss 5.483\n",
      "Epoch 3 Iteration 200: discriminator_loss 0.015 generator_loss 7.005\n",
      "Epoch 3 Iteration 300: discriminator_loss 0.059 generator_loss 7.296\n",
      "Epoch 3 Iteration 400: discriminator_loss 0.038 generator_loss 8.130\n",
      "Epoch 3 Iteration 500: discriminator_loss 0.003 generator_loss 10.940\n",
      "Epoch 3 Iteration 600: discriminator_loss 0.001 generator_loss 10.701\n",
      "Epoch 3 Iteration 700: discriminator_loss 0.036 generator_loss 7.116\n",
      "Epoch 3 Iteration 800: discriminator_loss 0.116 generator_loss 6.915\n",
      "Epoch 3 Iteration 900: discriminator_loss 0.200 generator_loss 4.540\n",
      "Epoch 3 Iteration 938: discriminator_loss 0.096 generator_loss 6.200\n",
      "Epoch 4 Iteration 100: discriminator_loss 0.033 generator_loss 6.958\n",
      "Epoch 4 Iteration 200: discriminator_loss 0.004 generator_loss 8.184\n",
      "Epoch 4 Iteration 300: discriminator_loss 0.062 generator_loss 16.455\n",
      "Epoch 4 Iteration 400: discriminator_loss 0.049 generator_loss 7.259\n",
      "Epoch 4 Iteration 500: discriminator_loss 0.066 generator_loss 5.681\n",
      "Epoch 4 Iteration 600: discriminator_loss 0.059 generator_loss 7.307\n",
      "Epoch 4 Iteration 700: discriminator_loss 0.034 generator_loss 6.852\n",
      "Epoch 4 Iteration 800: discriminator_loss 0.027 generator_loss 7.452\n",
      "Epoch 4 Iteration 900: discriminator_loss 0.000 generator_loss 21.944\n",
      "Epoch 4 Iteration 938: discriminator_loss 0.286 generator_loss 12.441\n",
      "Model saved.\n",
      "Epoch 5 Iteration 100: discriminator_loss 0.061 generator_loss 5.196\n",
      "Epoch 5 Iteration 200: discriminator_loss 0.036 generator_loss 5.854\n",
      "Epoch 5 Iteration 300: discriminator_loss 0.062 generator_loss 6.088\n",
      "Epoch 5 Iteration 400: discriminator_loss 0.119 generator_loss 9.338\n",
      "Epoch 5 Iteration 500: discriminator_loss 0.164 generator_loss 3.400\n",
      "Epoch 5 Iteration 600: discriminator_loss 0.033 generator_loss 4.327\n",
      "Epoch 5 Iteration 700: discriminator_loss 0.016 generator_loss 5.585\n",
      "Epoch 5 Iteration 800: discriminator_loss 0.011 generator_loss 6.255\n",
      "Epoch 5 Iteration 900: discriminator_loss 0.182 generator_loss 15.598\n",
      "Epoch 5 Iteration 938: discriminator_loss 0.474 generator_loss 18.417\n",
      "Epoch 6 Iteration 100: discriminator_loss 0.069 generator_loss 6.044\n",
      "Epoch 6 Iteration 200: discriminator_loss 0.033 generator_loss 7.662\n",
      "Epoch 6 Iteration 300: discriminator_loss 0.029 generator_loss 4.944\n",
      "Epoch 6 Iteration 400: discriminator_loss 0.016 generator_loss 6.129\n",
      "Epoch 6 Iteration 500: discriminator_loss 0.004 generator_loss 7.435\n",
      "Epoch 6 Iteration 600: discriminator_loss 0.001 generator_loss 9.042\n",
      "Epoch 6 Iteration 700: discriminator_loss 0.079 generator_loss 10.670\n",
      "Epoch 6 Iteration 800: discriminator_loss 0.030 generator_loss 9.461\n",
      "Epoch 6 Iteration 900: discriminator_loss 0.032 generator_loss 5.661\n",
      "Epoch 6 Iteration 938: discriminator_loss 0.003 generator_loss 6.644\n",
      "Epoch 7 Iteration 100: discriminator_loss 0.004 generator_loss 6.149\n",
      "Epoch 7 Iteration 200: discriminator_loss 0.028 generator_loss 5.729\n",
      "Epoch 7 Iteration 300: discriminator_loss 0.002 generator_loss 6.540\n",
      "Epoch 7 Iteration 400: discriminator_loss 0.001 generator_loss 8.242\n",
      "Epoch 7 Iteration 500: discriminator_loss 0.011 generator_loss 5.098\n",
      "Epoch 7 Iteration 600: discriminator_loss 0.008 generator_loss 6.703\n",
      "Epoch 7 Iteration 700: discriminator_loss 0.230 generator_loss 5.618\n",
      "Epoch 7 Iteration 800: discriminator_loss 0.079 generator_loss 5.874\n",
      "Epoch 7 Iteration 900: discriminator_loss 0.148 generator_loss 7.266\n",
      "Epoch 7 Iteration 938: discriminator_loss 0.010 generator_loss 7.752\n",
      "Epoch 8 Iteration 100: discriminator_loss 0.284 generator_loss 3.856\n",
      "Epoch 8 Iteration 200: discriminator_loss 0.033 generator_loss 5.084\n",
      "Epoch 8 Iteration 300: discriminator_loss 0.022 generator_loss 7.192\n",
      "Epoch 8 Iteration 400: discriminator_loss 0.022 generator_loss 13.661\n",
      "Epoch 8 Iteration 500: discriminator_loss 0.028 generator_loss 5.015\n",
      "Epoch 8 Iteration 600: discriminator_loss 0.015 generator_loss 5.155\n",
      "Epoch 8 Iteration 700: discriminator_loss 0.023 generator_loss 7.473\n",
      "Epoch 8 Iteration 800: discriminator_loss 0.167 generator_loss 5.824\n",
      "Epoch 8 Iteration 900: discriminator_loss 0.014 generator_loss 7.900\n",
      "Epoch 8 Iteration 938: discriminator_loss 0.030 generator_loss 7.404\n",
      "Epoch 9 Iteration 100: discriminator_loss 0.006 generator_loss 8.610\n",
      "Epoch 9 Iteration 200: discriminator_loss 0.017 generator_loss 5.399\n",
      "Epoch 9 Iteration 300: discriminator_loss 0.006 generator_loss 6.602\n",
      "Epoch 9 Iteration 400: discriminator_loss 0.065 generator_loss 6.597\n",
      "Epoch 9 Iteration 500: discriminator_loss 0.161 generator_loss 5.616\n",
      "Epoch 9 Iteration 600: discriminator_loss 0.004 generator_loss 9.359\n",
      "Epoch 9 Iteration 700: discriminator_loss 0.002 generator_loss 7.700\n",
      "Epoch 9 Iteration 800: discriminator_loss 0.028 generator_loss 5.170\n",
      "Epoch 9 Iteration 900: discriminator_loss 0.039 generator_loss 5.759\n",
      "Epoch 9 Iteration 938: discriminator_loss 0.025 generator_loss 7.146\n",
      "Model saved.\n",
      "Epoch 10 Iteration 100: discriminator_loss 0.009 generator_loss 6.319\n",
      "Epoch 10 Iteration 200: discriminator_loss 0.015 generator_loss 8.719\n",
      "Epoch 10 Iteration 300: discriminator_loss 0.210 generator_loss 4.371\n",
      "Epoch 10 Iteration 400: discriminator_loss 0.023 generator_loss 8.380\n",
      "Epoch 10 Iteration 500: discriminator_loss 0.002 generator_loss 9.024\n",
      "Epoch 10 Iteration 600: discriminator_loss 0.006 generator_loss 5.574\n",
      "Epoch 10 Iteration 700: discriminator_loss 0.002 generator_loss 6.234\n",
      "Epoch 10 Iteration 800: discriminator_loss 0.000 generator_loss 16.486\n",
      "Epoch 10 Iteration 900: discriminator_loss 0.178 generator_loss 17.361\n",
      "Epoch 10 Iteration 938: discriminator_loss 0.027 generator_loss 6.603\n",
      "Epoch 11 Iteration 100: discriminator_loss 0.040 generator_loss 6.258\n",
      "Epoch 11 Iteration 200: discriminator_loss 0.009 generator_loss 7.891\n",
      "Epoch 11 Iteration 300: discriminator_loss 0.006 generator_loss 7.184\n",
      "Epoch 11 Iteration 400: discriminator_loss 0.041 generator_loss 7.703\n",
      "Epoch 11 Iteration 500: discriminator_loss 0.001 generator_loss 11.296\n",
      "Epoch 11 Iteration 600: discriminator_loss 0.019 generator_loss 9.192\n",
      "Epoch 11 Iteration 700: discriminator_loss 0.055 generator_loss 5.191\n",
      "Epoch 11 Iteration 800: discriminator_loss 0.054 generator_loss 7.640\n",
      "Epoch 11 Iteration 900: discriminator_loss 0.020 generator_loss 9.109\n",
      "Epoch 11 Iteration 938: discriminator_loss 0.021 generator_loss 5.755\n",
      "Epoch 12 Iteration 100: discriminator_loss 0.006 generator_loss 6.523\n",
      "Epoch 12 Iteration 200: discriminator_loss 0.093 generator_loss 5.218\n",
      "Epoch 12 Iteration 300: discriminator_loss 0.007 generator_loss 7.048\n",
      "Epoch 12 Iteration 400: discriminator_loss 0.003 generator_loss 6.674\n",
      "Epoch 12 Iteration 500: discriminator_loss 0.118 generator_loss 8.641\n",
      "Epoch 12 Iteration 600: discriminator_loss 0.198 generator_loss 4.315\n",
      "Epoch 12 Iteration 700: discriminator_loss 0.032 generator_loss 7.942\n",
      "Epoch 12 Iteration 800: discriminator_loss 0.025 generator_loss 7.381\n",
      "Epoch 12 Iteration 900: discriminator_loss 0.039 generator_loss 4.896\n",
      "Epoch 12 Iteration 938: discriminator_loss 0.025 generator_loss 5.780\n",
      "Epoch 13 Iteration 100: discriminator_loss 0.002 generator_loss 8.490\n",
      "Epoch 13 Iteration 200: discriminator_loss 0.016 generator_loss 5.339\n",
      "Epoch 13 Iteration 300: discriminator_loss 0.020 generator_loss 5.163\n",
      "Epoch 13 Iteration 400: discriminator_loss 0.007 generator_loss 6.480\n",
      "Epoch 13 Iteration 500: discriminator_loss 0.193 generator_loss 8.863\n",
      "Epoch 13 Iteration 600: discriminator_loss 0.042 generator_loss 3.110\n",
      "Epoch 13 Iteration 700: discriminator_loss 0.031 generator_loss 6.709\n",
      "Epoch 13 Iteration 800: discriminator_loss 0.047 generator_loss 7.057\n",
      "Epoch 13 Iteration 900: discriminator_loss 0.041 generator_loss 6.609\n",
      "Epoch 13 Iteration 938: discriminator_loss 0.140 generator_loss 5.823\n",
      "Epoch 14 Iteration 100: discriminator_loss 0.033 generator_loss 5.486\n",
      "Epoch 14 Iteration 200: discriminator_loss 0.092 generator_loss 5.391\n",
      "Epoch 14 Iteration 300: discriminator_loss 0.032 generator_loss 5.310\n",
      "Epoch 14 Iteration 400: discriminator_loss 0.014 generator_loss 5.436\n",
      "Epoch 14 Iteration 500: discriminator_loss 0.067 generator_loss 7.098\n",
      "Epoch 14 Iteration 600: discriminator_loss 0.074 generator_loss 7.015\n",
      "Epoch 14 Iteration 700: discriminator_loss 0.064 generator_loss 5.842\n",
      "Epoch 14 Iteration 800: discriminator_loss 0.003 generator_loss 7.295\n",
      "Epoch 14 Iteration 900: discriminator_loss 0.013 generator_loss 10.639\n",
      "Epoch 14 Iteration 938: discriminator_loss 0.236 generator_loss 8.207\n",
      "Model saved.\n",
      "Epoch 15 Iteration 100: discriminator_loss 0.024 generator_loss 4.579\n",
      "Epoch 15 Iteration 200: discriminator_loss 0.006 generator_loss 5.946\n",
      "Epoch 15 Iteration 300: discriminator_loss 0.014 generator_loss 5.386\n",
      "Epoch 15 Iteration 400: discriminator_loss 0.092 generator_loss 10.119\n",
      "Epoch 15 Iteration 500: discriminator_loss 0.025 generator_loss 6.299\n",
      "Epoch 15 Iteration 600: discriminator_loss 0.006 generator_loss 4.403\n",
      "Epoch 15 Iteration 700: discriminator_loss 0.072 generator_loss 6.586\n",
      "Epoch 15 Iteration 800: discriminator_loss 0.025 generator_loss 5.107\n",
      "Epoch 15 Iteration 900: discriminator_loss 0.081 generator_loss 4.263\n",
      "Epoch 15 Iteration 938: discriminator_loss 0.052 generator_loss 3.780\n",
      "Epoch 16 Iteration 100: discriminator_loss 0.035 generator_loss 5.198\n",
      "Epoch 16 Iteration 200: discriminator_loss 0.015 generator_loss 12.389\n",
      "Epoch 16 Iteration 300: discriminator_loss 0.026 generator_loss 6.841\n",
      "Epoch 16 Iteration 400: discriminator_loss 0.073 generator_loss 4.171\n",
      "Epoch 16 Iteration 500: discriminator_loss 0.026 generator_loss 7.899\n",
      "Epoch 16 Iteration 600: discriminator_loss 0.002 generator_loss 12.737\n",
      "Epoch 16 Iteration 700: discriminator_loss 0.135 generator_loss 5.794\n",
      "Epoch 16 Iteration 800: discriminator_loss 0.041 generator_loss 7.406\n",
      "Epoch 16 Iteration 900: discriminator_loss 0.042 generator_loss 5.307\n",
      "Epoch 16 Iteration 938: discriminator_loss 0.029 generator_loss 7.272\n",
      "Epoch 17 Iteration 100: discriminator_loss 0.315 generator_loss 13.226\n",
      "Epoch 17 Iteration 200: discriminator_loss 0.065 generator_loss 7.300\n",
      "Epoch 17 Iteration 300: discriminator_loss 0.050 generator_loss 6.796\n",
      "Epoch 17 Iteration 400: discriminator_loss 0.022 generator_loss 9.326\n",
      "Epoch 17 Iteration 500: discriminator_loss 0.012 generator_loss 5.813\n",
      "Epoch 17 Iteration 600: discriminator_loss 0.010 generator_loss 5.350\n",
      "Epoch 17 Iteration 700: discriminator_loss 0.027 generator_loss 6.087\n",
      "Epoch 17 Iteration 800: discriminator_loss 0.004 generator_loss 10.150\n",
      "Epoch 17 Iteration 900: discriminator_loss 0.068 generator_loss 5.635\n",
      "Epoch 17 Iteration 938: discriminator_loss 0.048 generator_loss 7.674\n",
      "Epoch 18 Iteration 100: discriminator_loss 0.019 generator_loss 6.148\n",
      "Epoch 18 Iteration 200: discriminator_loss 0.024 generator_loss 6.682\n",
      "Epoch 18 Iteration 300: discriminator_loss 0.181 generator_loss 4.599\n",
      "Epoch 18 Iteration 400: discriminator_loss 0.012 generator_loss 6.956\n",
      "Epoch 18 Iteration 500: discriminator_loss 0.037 generator_loss 6.705\n",
      "Epoch 18 Iteration 600: discriminator_loss 0.194 generator_loss 6.317\n",
      "Epoch 18 Iteration 700: discriminator_loss 0.091 generator_loss 4.573\n",
      "Epoch 18 Iteration 800: discriminator_loss 0.029 generator_loss 7.239\n",
      "Epoch 18 Iteration 900: discriminator_loss 0.024 generator_loss 5.445\n",
      "Epoch 18 Iteration 938: discriminator_loss 0.019 generator_loss 5.637\n",
      "Epoch 19 Iteration 100: discriminator_loss 0.077 generator_loss 9.478\n",
      "Epoch 19 Iteration 200: discriminator_loss 0.078 generator_loss 7.331\n",
      "Epoch 19 Iteration 300: discriminator_loss 0.119 generator_loss 5.647\n",
      "Epoch 19 Iteration 400: discriminator_loss 0.033 generator_loss 3.821\n",
      "Epoch 19 Iteration 500: discriminator_loss 0.232 generator_loss 3.607\n",
      "Epoch 19 Iteration 600: discriminator_loss 0.013 generator_loss 10.091\n",
      "Epoch 19 Iteration 700: discriminator_loss 0.122 generator_loss 6.047\n",
      "Epoch 19 Iteration 800: discriminator_loss 0.097 generator_loss 4.821\n",
      "Epoch 19 Iteration 900: discriminator_loss 0.049 generator_loss 5.039\n",
      "Epoch 19 Iteration 938: discriminator_loss 0.003 generator_loss 9.191\n",
      "Model saved.\n",
      "Epoch 20 Iteration 100: discriminator_loss 0.058 generator_loss 6.161\n",
      "Epoch 20 Iteration 200: discriminator_loss 0.039 generator_loss 6.848\n",
      "Epoch 20 Iteration 300: discriminator_loss 0.014 generator_loss 5.348\n",
      "Epoch 20 Iteration 400: discriminator_loss 0.077 generator_loss 4.046\n",
      "Epoch 20 Iteration 500: discriminator_loss 0.041 generator_loss 4.398\n",
      "Epoch 20 Iteration 600: discriminator_loss 0.028 generator_loss 7.486\n",
      "Epoch 20 Iteration 700: discriminator_loss 0.072 generator_loss 8.400\n",
      "Epoch 20 Iteration 800: discriminator_loss 0.078 generator_loss 3.966\n",
      "Epoch 20 Iteration 900: discriminator_loss 0.033 generator_loss 6.668\n",
      "Epoch 20 Iteration 938: discriminator_loss 0.088 generator_loss 8.493\n",
      "Epoch 21 Iteration 100: discriminator_loss 0.086 generator_loss 6.950\n",
      "Epoch 21 Iteration 200: discriminator_loss 0.039 generator_loss 4.829\n",
      "Epoch 21 Iteration 300: discriminator_loss 0.009 generator_loss 7.248\n",
      "Epoch 21 Iteration 400: discriminator_loss 0.042 generator_loss 4.529\n",
      "Epoch 21 Iteration 500: discriminator_loss 0.058 generator_loss 6.706\n",
      "Epoch 21 Iteration 600: discriminator_loss 0.016 generator_loss 8.797\n",
      "Epoch 21 Iteration 700: discriminator_loss 0.028 generator_loss 6.325\n",
      "Epoch 21 Iteration 800: discriminator_loss 0.048 generator_loss 4.587\n",
      "Epoch 21 Iteration 900: discriminator_loss 0.010 generator_loss 8.930\n",
      "Epoch 21 Iteration 938: discriminator_loss 0.041 generator_loss 7.146\n",
      "Epoch 22 Iteration 100: discriminator_loss 0.118 generator_loss 5.568\n",
      "Epoch 22 Iteration 200: discriminator_loss 0.073 generator_loss 5.803\n",
      "Epoch 22 Iteration 300: discriminator_loss 0.031 generator_loss 6.599\n",
      "Epoch 22 Iteration 400: discriminator_loss 0.091 generator_loss 3.977\n",
      "Epoch 22 Iteration 500: discriminator_loss 0.063 generator_loss 5.465\n",
      "Epoch 22 Iteration 600: discriminator_loss 0.039 generator_loss 7.551\n",
      "Epoch 22 Iteration 700: discriminator_loss 0.102 generator_loss 4.615\n",
      "Epoch 22 Iteration 800: discriminator_loss 0.039 generator_loss 7.071\n",
      "Epoch 22 Iteration 900: discriminator_loss 0.028 generator_loss 4.929\n",
      "Epoch 22 Iteration 938: discriminator_loss 0.136 generator_loss 4.959\n",
      "Epoch 23 Iteration 100: discriminator_loss 0.066 generator_loss 6.028\n",
      "Epoch 23 Iteration 200: discriminator_loss 0.055 generator_loss 6.678\n",
      "Epoch 23 Iteration 300: discriminator_loss 0.042 generator_loss 5.047\n",
      "Epoch 23 Iteration 400: discriminator_loss 0.061 generator_loss 5.848\n",
      "Epoch 23 Iteration 500: discriminator_loss 0.112 generator_loss 4.568\n",
      "Epoch 23 Iteration 600: discriminator_loss 0.100 generator_loss 5.132\n",
      "Epoch 23 Iteration 700: discriminator_loss 0.205 generator_loss 5.461\n",
      "Epoch 23 Iteration 800: discriminator_loss 0.037 generator_loss 5.743\n",
      "Epoch 23 Iteration 900: discriminator_loss 0.123 generator_loss 4.701\n",
      "Epoch 23 Iteration 938: discriminator_loss 0.103 generator_loss 6.413\n",
      "Epoch 24 Iteration 100: discriminator_loss 0.170 generator_loss 5.185\n",
      "Epoch 24 Iteration 200: discriminator_loss 0.196 generator_loss 5.508\n",
      "Epoch 24 Iteration 300: discriminator_loss 0.089 generator_loss 3.422\n",
      "Epoch 24 Iteration 400: discriminator_loss 0.091 generator_loss 4.413\n",
      "Epoch 24 Iteration 500: discriminator_loss 0.153 generator_loss 3.280\n",
      "Epoch 24 Iteration 600: discriminator_loss 0.095 generator_loss 3.787\n",
      "Epoch 24 Iteration 700: discriminator_loss 0.121 generator_loss 3.403\n",
      "Epoch 24 Iteration 800: discriminator_loss 0.202 generator_loss 4.066\n",
      "Epoch 24 Iteration 900: discriminator_loss 0.188 generator_loss 5.081\n",
      "Epoch 24 Iteration 938: discriminator_loss 0.094 generator_loss 3.912\n",
      "Model saved.\n",
      "Epoch 25 Iteration 100: discriminator_loss 0.135 generator_loss 3.503\n",
      "Epoch 25 Iteration 200: discriminator_loss 0.188 generator_loss 3.203\n",
      "Epoch 25 Iteration 300: discriminator_loss 0.084 generator_loss 3.702\n",
      "Epoch 25 Iteration 400: discriminator_loss 0.174 generator_loss 4.083\n",
      "Epoch 25 Iteration 500: discriminator_loss 0.144 generator_loss 3.586\n",
      "Epoch 25 Iteration 600: discriminator_loss 0.137 generator_loss 3.866\n",
      "Epoch 25 Iteration 700: discriminator_loss 0.287 generator_loss 4.797\n",
      "Epoch 25 Iteration 800: discriminator_loss 0.185 generator_loss 2.497\n",
      "Epoch 25 Iteration 900: discriminator_loss 0.176 generator_loss 2.598\n",
      "Epoch 25 Iteration 938: discriminator_loss 0.098 generator_loss 4.628\n",
      "Epoch 26 Iteration 100: discriminator_loss 0.114 generator_loss 3.978\n",
      "Epoch 26 Iteration 200: discriminator_loss 0.167 generator_loss 5.705\n",
      "Epoch 26 Iteration 300: discriminator_loss 0.130 generator_loss 5.032\n",
      "Epoch 26 Iteration 400: discriminator_loss 0.285 generator_loss 3.250\n",
      "Epoch 26 Iteration 500: discriminator_loss 0.110 generator_loss 3.249\n",
      "Epoch 26 Iteration 600: discriminator_loss 0.085 generator_loss 4.210\n",
      "Epoch 26 Iteration 700: discriminator_loss 0.191 generator_loss 3.285\n",
      "Epoch 26 Iteration 800: discriminator_loss 0.102 generator_loss 4.184\n",
      "Epoch 26 Iteration 900: discriminator_loss 0.205 generator_loss 1.597\n",
      "Epoch 26 Iteration 938: discriminator_loss 0.131 generator_loss 3.084\n",
      "Epoch 27 Iteration 100: discriminator_loss 0.152 generator_loss 3.167\n",
      "Epoch 27 Iteration 200: discriminator_loss 0.212 generator_loss 3.607\n",
      "Epoch 27 Iteration 300: discriminator_loss 0.265 generator_loss 3.365\n",
      "Epoch 27 Iteration 400: discriminator_loss 0.237 generator_loss 4.002\n",
      "Epoch 27 Iteration 500: discriminator_loss 0.142 generator_loss 2.940\n",
      "Epoch 27 Iteration 600: discriminator_loss 0.098 generator_loss 4.416\n",
      "Epoch 27 Iteration 700: discriminator_loss 0.275 generator_loss 3.052\n",
      "Epoch 27 Iteration 800: discriminator_loss 0.130 generator_loss 2.852\n",
      "Epoch 27 Iteration 900: discriminator_loss 0.167 generator_loss 2.831\n",
      "Epoch 27 Iteration 938: discriminator_loss 0.203 generator_loss 4.520\n",
      "Epoch 28 Iteration 100: discriminator_loss 0.188 generator_loss 3.873\n",
      "Epoch 28 Iteration 200: discriminator_loss 0.215 generator_loss 3.980\n",
      "Epoch 28 Iteration 300: discriminator_loss 0.179 generator_loss 3.453\n",
      "Epoch 28 Iteration 400: discriminator_loss 0.221 generator_loss 2.447\n",
      "Epoch 28 Iteration 500: discriminator_loss 0.204 generator_loss 4.287\n",
      "Epoch 28 Iteration 600: discriminator_loss 0.102 generator_loss 3.705\n",
      "Epoch 28 Iteration 700: discriminator_loss 0.128 generator_loss 3.792\n",
      "Epoch 28 Iteration 800: discriminator_loss 0.146 generator_loss 3.928\n",
      "Epoch 28 Iteration 900: discriminator_loss 0.115 generator_loss 3.771\n",
      "Epoch 28 Iteration 938: discriminator_loss 0.290 generator_loss 2.863\n",
      "Epoch 29 Iteration 100: discriminator_loss 0.155 generator_loss 1.843\n",
      "Epoch 29 Iteration 200: discriminator_loss 0.143 generator_loss 3.266\n",
      "Epoch 29 Iteration 300: discriminator_loss 0.179 generator_loss 3.580\n",
      "Epoch 29 Iteration 400: discriminator_loss 0.327 generator_loss 2.979\n",
      "Epoch 29 Iteration 500: discriminator_loss 0.190 generator_loss 2.460\n",
      "Epoch 29 Iteration 600: discriminator_loss 0.164 generator_loss 3.011\n",
      "Epoch 29 Iteration 700: discriminator_loss 0.102 generator_loss 5.482\n",
      "Epoch 29 Iteration 800: discriminator_loss 0.137 generator_loss 3.540\n",
      "Epoch 29 Iteration 900: discriminator_loss 0.150 generator_loss 2.188\n",
      "Epoch 29 Iteration 938: discriminator_loss 0.188 generator_loss 2.619\n",
      "Model saved.\n",
      "Epoch 30 Iteration 100: discriminator_loss 0.269 generator_loss 2.989\n",
      "Epoch 30 Iteration 200: discriminator_loss 0.188 generator_loss 3.367\n",
      "Epoch 30 Iteration 300: discriminator_loss 0.190 generator_loss 3.649\n",
      "Epoch 30 Iteration 400: discriminator_loss 0.128 generator_loss 2.464\n",
      "Epoch 30 Iteration 500: discriminator_loss 0.194 generator_loss 2.713\n",
      "Epoch 30 Iteration 600: discriminator_loss 0.218 generator_loss 2.282\n",
      "Epoch 30 Iteration 700: discriminator_loss 0.189 generator_loss 3.559\n",
      "Epoch 30 Iteration 800: discriminator_loss 0.174 generator_loss 3.263\n",
      "Epoch 30 Iteration 900: discriminator_loss 0.215 generator_loss 2.594\n",
      "Epoch 30 Iteration 938: discriminator_loss 0.203 generator_loss 3.734\n",
      "Epoch 31 Iteration 100: discriminator_loss 0.260 generator_loss 3.940\n",
      "Epoch 31 Iteration 200: discriminator_loss 0.358 generator_loss 2.695\n",
      "Epoch 31 Iteration 300: discriminator_loss 0.256 generator_loss 2.778\n",
      "Epoch 31 Iteration 400: discriminator_loss 0.205 generator_loss 2.907\n",
      "Epoch 31 Iteration 500: discriminator_loss 0.135 generator_loss 2.781\n",
      "Epoch 31 Iteration 600: discriminator_loss 0.220 generator_loss 4.543\n",
      "Epoch 31 Iteration 700: discriminator_loss 0.187 generator_loss 2.858\n",
      "Epoch 31 Iteration 800: discriminator_loss 0.175 generator_loss 3.430\n",
      "Epoch 31 Iteration 900: discriminator_loss 0.256 generator_loss 1.873\n",
      "Epoch 31 Iteration 938: discriminator_loss 0.209 generator_loss 3.091\n",
      "Epoch 32 Iteration 100: discriminator_loss 0.118 generator_loss 2.232\n",
      "Epoch 32 Iteration 200: discriminator_loss 0.217 generator_loss 3.334\n",
      "Epoch 32 Iteration 300: discriminator_loss 0.233 generator_loss 2.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(900):\n",
    "    for idx, (imgs, _) in enumerate(mnist_dataloader):\n",
    "        idx += 1\n",
    "        # Обучаем дискриминатор\n",
    "        # real_inputs - изображения из набора данных MNIST \n",
    "        # fake_inputs - изображения от генератора\n",
    "        # real_inputs должны быть классифицированы как 1, а fake_inputs - как 0\n",
    "        real_inputs = imgs.to(device)\n",
    "        real_outputs = D(real_inputs)\n",
    "        real_label = torch.ones(real_inputs.shape[0], 1).to(device)\n",
    "        noise = (torch.rand(real_inputs.shape[0], 128) - 0.5) / 0.5\n",
    "        noise = noise.to(device)\n",
    "        fake_inputs = G(noise)\n",
    "        fake_outputs = D(fake_inputs)\n",
    "        fake_label = torch.zeros(fake_inputs.shape[0], 1).to(device)\n",
    "        outputs = torch.cat((real_outputs, fake_outputs), 0)\n",
    "        targets = torch.cat((real_label, fake_label), 0)\n",
    "        D_loss = loss(outputs, targets)\n",
    "        D_optimizer.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "        # Обучаем генератор\n",
    "        # Цель генератора получить от дискриминатора 1 по всем изображениям\n",
    "        noise = (torch.rand(real_inputs.shape[0], 128) - 0.5) / 0.5\n",
    "        noise = noise.to(device)\n",
    "        fake_inputs = G(noise)\n",
    "        fake_outputs = D(fake_inputs)\n",
    "        fake_targets = torch.ones([fake_inputs.shape[0], 1]).to(device)\n",
    "        G_loss = loss(fake_outputs, fake_targets)\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        #print(D_loss.item(), G_loss.item())\n",
    "        if idx % 100 == 0 or idx == len(mnist_dataloader):\n",
    "            print('Epoch {} Iteration {}: discriminator_loss {:.3f} generator_loss {:.3f}'.format(epoch, idx,\n",
    "                                                                                                  D_loss.item(),\n",
    "                                                                                                  G_loss.item()))\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(G, './test/Generator_epoch_{}.pth'.format(epoch))\n",
    "        print('Model saved.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T14:42:19.709921Z",
     "start_time": "2024-01-31T14:37:29.199753200Z"
    }
   },
   "id": "729bbc3b34035501"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100, 1, 1])\n",
      "torch.Size([10, 1024, 4, 4])\n",
      "torch.Size([10, 512, 8, 8])\n",
      "torch.Size([10, 256, 16, 16])\n",
      "torch.Size([10, 128, 32, 32])\n",
      "torch.Size([10, 3, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[[-0.2782,  0.3445, -0.3528,  ...,  0.0174, -0.4197,  0.1874],\n          [-0.4489,  0.3890,  0.3467,  ..., -0.0972,  0.7847, -0.0382],\n          [-0.4859,  0.1005, -0.7718,  ...,  0.0491, -0.5042,  0.2689],\n          ...,\n          [-0.2143,  0.9263, -0.2161,  ...,  0.8581,  0.7893,  0.5876],\n          [-0.5816, -0.2697,  0.2265,  ...,  0.2883, -0.6874,  0.4437],\n          [ 0.0771,  0.2777, -0.0731,  ...,  0.3328,  0.0880, -0.0490]],\n\n         [[-0.3202,  0.2165, -0.0047,  ...,  0.6146, -0.3220,  0.1655],\n          [-0.4192,  0.0605,  0.2759,  ...,  0.7337,  0.1701,  0.3675],\n          [-0.6238, -0.1057, -0.1238,  ..., -0.0664, -0.0596, -0.0920],\n          ...,\n          [-0.0018,  0.7566,  0.7461,  ...,  0.9128,  0.1981,  0.5471],\n          [-0.2693,  0.2392, -0.6801,  ..., -0.5278, -0.5777,  0.1734],\n          [-0.3589,  0.1306, -0.5616,  ...,  0.5361, -0.1228,  0.1405]],\n\n         [[-0.1175,  0.2353, -0.1488,  ...,  0.2259, -0.1552,  0.2754],\n          [ 0.4418,  0.2368,  0.3850,  ..., -0.5165,  0.7212,  0.4930],\n          [ 0.2628,  0.4859,  0.5211,  ...,  0.4721,  0.7504,  0.0547],\n          ...,\n          [ 0.1155,  0.0680,  0.5047,  ..., -0.4322,  0.2503,  0.1804],\n          [-0.0408,  0.6619, -0.5894,  ...,  0.8637,  0.2093,  0.1839],\n          [ 0.2702, -0.1936,  0.2481,  ..., -0.3716,  0.3052,  0.1644]]],\n\n\n        [[[-0.3622,  0.1297, -0.3649,  ...,  0.1949, -0.6278,  0.1100],\n          [-0.3311,  0.5615,  0.7953,  ...,  0.2802,  0.7799, -0.1756],\n          [-0.2069,  0.1366, -0.8656,  ...,  0.2464, -0.3238,  0.3336],\n          ...,\n          [-0.1682,  0.9189, -0.6944,  ...,  0.7291,  0.9141,  0.6200],\n          [-0.4141, -0.2653, -0.1387,  ...,  0.1055, -0.4937,  0.1951],\n          [-0.1651,  0.2823, -0.1321,  ...,  0.0703, -0.0649, -0.1325]],\n\n         [[-0.2571,  0.2314, -0.0727,  ...,  0.6004, -0.4190,  0.1893],\n          [-0.5257,  0.4365,  0.6581,  ...,  0.5253, -0.3372,  0.2482],\n          [-0.6454, -0.1620, -0.3029,  ..., -0.3274, -0.1814, -0.0533],\n          ...,\n          [-0.0069,  0.6592,  0.1315,  ...,  0.8455, -0.0256,  0.4777],\n          [-0.2216,  0.0142, -0.0565,  ..., -0.2203, -0.4268,  0.2917],\n          [-0.3276,  0.3470, -0.5133,  ...,  0.6388, -0.0768,  0.1500]],\n\n         [[-0.1584,  0.2790, -0.1194,  ...,  0.3186, -0.1485,  0.1625],\n          [ 0.4869,  0.2864, -0.1227,  ..., -0.0472,  0.7723,  0.3619],\n          [ 0.1351,  0.4564,  0.4617,  ..., -0.2792,  0.2351, -0.1460],\n          ...,\n          [ 0.2172,  0.2125,  0.1234,  ...,  0.0803,  0.2763, -0.0277],\n          [ 0.1112,  0.7285, -0.5812,  ...,  0.8091,  0.0036,  0.0281],\n          [ 0.2957, -0.1429,  0.2797,  ..., -0.4859,  0.3853,  0.1864]]],\n\n\n        [[[-0.3485,  0.1124, -0.3153,  ...,  0.1423, -0.4185,  0.2687],\n          [-0.3412,  0.3959,  0.4762,  ...,  0.3360,  0.8683, -0.3290],\n          [-0.2169,  0.5293, -0.8393,  ..., -0.0689, -0.4606,  0.1471],\n          ...,\n          [-0.1109,  0.8774, -0.0971,  ...,  0.8763,  0.9307,  0.6325],\n          [-0.5089, -0.1608, -0.1193,  ...,  0.4000, -0.6657,  0.4756],\n          [-0.0632,  0.3581, -0.2076,  ...,  0.1471,  0.2365, -0.1115]],\n\n         [[-0.2765,  0.1107, -0.0905,  ...,  0.6478, -0.4453,  0.2861],\n          [-0.4045,  0.3093,  0.2698,  ...,  0.6841, -0.4055,  0.3284],\n          [-0.5775, -0.7010, -0.2450,  ..., -0.1201, -0.1278, -0.1183],\n          ...,\n          [ 0.0431,  0.7106,  0.5540,  ...,  0.8197,  0.0858,  0.5347],\n          [-0.1214, -0.3297, -0.2774,  ..., -0.3428, -0.2085,  0.2340],\n          [-0.3378,  0.2794, -0.5531,  ...,  0.4670, -0.2389,  0.0597]],\n\n         [[-0.1829,  0.3058, -0.2860,  ...,  0.1595, -0.1049,  0.1280],\n          [ 0.5261,  0.6489,  0.3150,  ..., -0.2522,  0.7515,  0.3650],\n          [ 0.2009,  0.5374,  0.8515,  ...,  0.1960,  0.8024,  0.0682],\n          ...,\n          [ 0.1918,  0.4052,  0.2291,  ...,  0.1576,  0.2175,  0.0299],\n          [-0.0163,  0.8269, -0.5750,  ...,  0.8237,  0.2139,  0.1047],\n          [ 0.2545, -0.0950,  0.4262,  ..., -0.4456,  0.3839,  0.2005]]],\n\n\n        ...,\n\n\n        [[[-0.4581,  0.1770, -0.3175,  ...,  0.0608, -0.5314,  0.1662],\n          [-0.3297,  0.1598,  0.7556,  ...,  0.2656,  0.9101, -0.2612],\n          [-0.2265,  0.0564, -0.8403,  ...,  0.1884, -0.3876,  0.1750],\n          ...,\n          [ 0.0497,  0.9646, -0.6546,  ...,  0.5989,  0.9201,  0.6598],\n          [-0.5489, -0.4394, -0.0910,  ...,  0.4440, -0.6133,  0.1956],\n          [-0.1339,  0.4759,  0.0182,  ...,  0.1138,  0.1138, -0.0919]],\n\n         [[-0.2426,  0.0697, -0.0745,  ...,  0.6213, -0.3623,  0.2068],\n          [-0.3838,  0.6457,  0.3833,  ...,  0.7088, -0.5333,  0.2611],\n          [-0.5171,  0.0752, -0.5721,  ...,  0.2529,  0.0604,  0.0273],\n          ...,\n          [-0.0391,  0.6175,  0.0245,  ...,  0.7795,  0.3896,  0.5745],\n          [-0.3960, -0.1970,  0.0894,  ..., -0.4738, -0.3296,  0.0092],\n          [-0.3611,  0.3167, -0.5634,  ...,  0.5494, -0.0288,  0.0414]],\n\n         [[-0.1631,  0.2527, -0.1495,  ...,  0.0888, -0.0594,  0.2061],\n          [ 0.4779,  0.3412, -0.0602,  ..., -0.0065,  0.8467,  0.2783],\n          [-0.0456,  0.2574,  0.6374,  ...,  0.0930,  0.6036,  0.0985],\n          ...,\n          [ 0.3657,  0.2892,  0.3345,  ..., -0.2265,  0.5760,  0.0368],\n          [ 0.0307,  0.7736, -0.7858,  ...,  0.8304, -0.0899,  0.2168],\n          [ 0.2954, -0.1677,  0.2832,  ..., -0.3305,  0.5581,  0.0893]]],\n\n\n        [[[-0.3397,  0.0160, -0.3228,  ...,  0.0385, -0.4698,  0.1454],\n          [-0.5294,  0.3841,  0.7553,  ...,  0.4846,  0.7909, -0.2057],\n          [-0.3988,  0.1418, -0.8722,  ...,  0.2597, -0.3151,  0.2163],\n          ...,\n          [-0.1299,  0.7879, -0.3232,  ...,  0.8231,  0.8727,  0.6491],\n          [-0.4023, -0.2626,  0.2068,  ...,  0.5316, -0.5412,  0.2525],\n          [-0.1688,  0.4830, -0.1726,  ...,  0.0958,  0.1742, -0.1429]],\n\n         [[-0.2740,  0.2992, -0.1364,  ...,  0.4107, -0.4084,  0.1971],\n          [-0.5325,  0.4173,  0.3805,  ...,  0.7488, -0.5044,  0.2050],\n          [-0.6713, -0.3286, -0.5344,  ..., -0.2067,  0.0965, -0.2676],\n          ...,\n          [-0.1535,  0.7646,  0.1152,  ...,  0.6867,  0.3948,  0.5171],\n          [-0.2773, -0.3840, -0.2541,  ..., -0.3502,  0.1770,  0.1979],\n          [-0.3339,  0.2808, -0.4920,  ...,  0.5988, -0.2294,  0.1095]],\n\n         [[-0.1219,  0.2760, -0.1147,  ...,  0.3044, -0.1669,  0.2663],\n          [ 0.4033,  0.3931, -0.1933,  ...,  0.1384,  0.8247,  0.1641],\n          [ 0.0772,  0.4855,  0.7744,  ...,  0.4591,  0.4288,  0.0214],\n          ...,\n          [ 0.0963,  0.3887,  0.3631,  ..., -0.3042,  0.5881,  0.1207],\n          [ 0.0388,  0.5734, -0.3851,  ...,  0.9008,  0.0970,  0.1298],\n          [ 0.2987, -0.1898,  0.3876,  ..., -0.5371,  0.3584,  0.1548]]],\n\n\n        [[[-0.3456,  0.0225, -0.2227,  ...,  0.1048, -0.5868,  0.1105],\n          [-0.3606, -0.0059,  0.2187,  ...,  0.5382,  0.8664, -0.3363],\n          [-0.3222,  0.5689, -0.8634,  ...,  0.1135, -0.3020,  0.1272],\n          ...,\n          [ 0.0143,  0.9489, -0.0606,  ...,  0.8757,  0.9289,  0.6176],\n          [-0.5513, -0.4261,  0.2402,  ...,  0.4843, -0.1301,  0.1733],\n          [-0.1136,  0.3226,  0.0230,  ...,  0.1173,  0.1102, -0.0227]],\n\n         [[-0.3050,  0.1172, -0.1705,  ...,  0.4009, -0.5961,  0.2018],\n          [-0.4088,  0.3755,  0.1243,  ...,  0.6895,  0.1462,  0.2135],\n          [-0.3604, -0.4596, -0.2040,  ...,  0.1089,  0.2162, -0.3543],\n          ...,\n          [-0.0491,  0.3269, -0.0518,  ...,  0.9351, -0.1829,  0.5352],\n          [-0.3485, -0.0816, -0.3313,  ..., -0.5110, -0.1451,  0.2772],\n          [-0.2786,  0.3640, -0.4005,  ...,  0.6421, -0.0693,  0.1051]],\n\n         [[-0.0681,  0.4853, -0.1914,  ...,  0.0439, -0.1830,  0.1370],\n          [ 0.4500,  0.3912,  0.3640,  ..., -0.2347,  0.8348,  0.3597],\n          [ 0.1912, -0.3692,  0.8743,  ...,  0.2816,  0.1475,  0.0726],\n          ...,\n          [ 0.3707,  0.0861,  0.3799,  ..., -0.2343,  0.5051,  0.1448],\n          [-0.0643,  0.8715, -0.7101,  ...,  0.9041,  0.4043,  0.1325],\n          [ 0.3622, -0.0456,  0.3559,  ..., -0.3090,  0.3720,  0.2192]]]],\n       grad_fn=<TanhBackward0>)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = nn.Sequential(\n",
    "    nn.ConvTranspose2d(in_channels=100, out_channels=1024, stride=1, kernel_size=4, padding=0, bias=False),\n",
    "    nn.BatchNorm2d(num_features=1024),\n",
    "    nn.ReLU(inplace=True), )\n",
    "\n",
    "layer2 = nn.Sequential(\n",
    "    nn.ConvTranspose2d(in_channels=1024, out_channels=512, stride=2, kernel_size=4, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(num_features=512),\n",
    "    nn.ReLU(inplace=True), )\n",
    "layer3 = nn.Sequential(\n",
    "    nn.ConvTranspose2d(in_channels=512, out_channels=256, stride=2, kernel_size=4, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(num_features=256),\n",
    "    nn.ReLU(inplace=True), )\n",
    "layer4 = nn.Sequential(\n",
    "    nn.ConvTranspose2d(in_channels=256, out_channels=128, stride=2, kernel_size=4, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(num_features=128),\n",
    "    nn.ReLU(inplace=True), )\n",
    "layer5 = nn.Sequential(\n",
    "    nn.ConvTranspose2d(in_channels=128, out_channels=3, stride=2, kernel_size=4, padding=1, bias=False),\n",
    "    nn.Tanh())\n",
    "x = torch.rand(10, 100)\n",
    "x = x.reshape((x.shape[0], x.shape[1], 1, 1))\n",
    "print(x.size())\n",
    "print(layer1(x).size())\n",
    "x = layer1(x)\n",
    "print(layer2(x).size())\n",
    "x = layer2(x)\n",
    "print(layer3(x).size())\n",
    "x = layer3(x)\n",
    "print(layer4(x).size())\n",
    "x = layer4(x)\n",
    "print(layer5(x).size())\n",
    "layer5(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T06:30:03.810949300Z",
     "start_time": "2024-02-02T06:30:03.723887100Z"
    }
   },
   "id": "744cebbebec4ac8",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://www.kaggle.com/code/vyacheslavshen/dcgan-pytorch-tutorial"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87aaf2a6f372a82e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.clayer1 = nn.Sequential(nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "                                     nn.LeakyReLU(),\n",
    "                                     nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                                     )\n",
    "        self.clayer2 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "                                     nn.LeakyReLU(),\n",
    "                                     nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 1024)\n",
    "        self.ac1 = torch.nn.LeakyReLU()\n",
    "        self.fc2 = torch.nn.Linear(1024, 256)\n",
    "        self.ac2 = torch.nn.LeakyReLU()\n",
    "        self.fc3 = torch.nn.Linear(256, 10)\n",
    "        self.ac_end = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.clayer1(x)\n",
    "        x = self.clayer2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ac1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.ac2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.ac_end(x)\n",
    "        return x\n",
    "\n",
    "    def inference(self, x):\n",
    "        x = self.forward(x)\n",
    "        return nn.Softmax(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "316e151be028f5d9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m6\u001B[39m))\n\u001B[1;32m----> 3\u001B[0m noise \u001B[38;5;241m=\u001B[39m (torch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m128\u001B[39m) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m0.5\u001B[39m) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m0.5\u001B[39m\n\u001B[0;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mtick_params(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mboth\u001B[39m\u001B[38;5;124m'\u001B[39m, length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39maxis(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moff\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 600x600 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "noise = (torch.rand(1, 128) - 0.5) / 0.5\n",
    "plt.tick_params(axis='both', length=0)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(G((torch.rand(1, 128) - 0.5) / 0.5)[0].detach().numpy().transpose((1, 2, 0)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T09:02:06.614332400Z",
     "start_time": "2024-02-01T09:02:06.053815400Z"
    }
   },
   "id": "13e9056d6b760fa2",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[-0.88548404],\n        [-0.7913319 ],\n        [-0.9817679 ],\n        [-0.94669956],\n        [-0.9718603 ],\n        [-0.98320305],\n        [-0.9967326 ],\n        [-0.99200535],\n        [-0.9570636 ],\n        [ 0.7909233 ],\n        [-0.9638806 ],\n        [-0.97846156],\n        [-0.97229147],\n        [-0.60605186],\n        [-0.84565663],\n        [-0.98283106],\n        [-0.9956229 ],\n        [-0.7313035 ],\n        [-0.93295574],\n        [-0.82077456],\n        [-0.7143531 ],\n        [-0.89652544],\n        [-0.95414615],\n        [ 0.57141596],\n        [-0.9575322 ],\n        [-0.9831623 ],\n        [-0.97540027],\n        [-0.9885837 ]],\n\n       [[-0.80315375],\n        [-0.94764155],\n        [-0.9790268 ],\n        [-0.98374903],\n        [-0.93665755],\n        [-0.9983478 ],\n        [-0.9332086 ],\n        [-0.8807999 ],\n        [ 0.82356393],\n        [-0.8804912 ],\n        [-0.93328255],\n        [-0.9922101 ],\n        [-0.58310944],\n        [-0.9806069 ],\n        [-0.99423444],\n        [-0.985275  ],\n        [-0.99396425],\n        [-0.9655599 ],\n        [-0.9732062 ],\n        [-0.9605119 ],\n        [-0.8181235 ],\n        [-0.9818953 ],\n        [ 0.02325944],\n        [-0.92335695],\n        [-0.59984654],\n        [-0.86818695],\n        [-0.9693825 ],\n        [-0.9920777 ]],\n\n       [[ 0.06921522],\n        [-0.9157511 ],\n        [-0.9863412 ],\n        [-0.9661986 ],\n        [-0.78650165],\n        [-0.7784134 ],\n        [-0.98793054],\n        [-0.90754783],\n        [-0.9755843 ],\n        [-0.7472082 ],\n        [-0.9579288 ],\n        [-0.8168505 ],\n        [-0.99583685],\n        [ 0.08137682],\n        [-0.93104315],\n        [-0.9745083 ],\n        [-0.9329671 ],\n        [-0.9083016 ],\n        [-0.99515283],\n        [-0.8134289 ],\n        [-0.99786127],\n        [-0.42106238],\n        [-0.67473733],\n        [-0.5434468 ],\n        [-0.79966366],\n        [-0.97013867],\n        [-0.99462634],\n        [-0.9823626 ]],\n\n       [[-0.9573646 ],\n        [-0.94205016],\n        [-0.99295866],\n        [-0.8270583 ],\n        [-0.9583059 ],\n        [-0.9303675 ],\n        [-0.9926005 ],\n        [-0.97257185],\n        [-0.89356285],\n        [ 0.7047832 ],\n        [-0.88449025],\n        [-0.11728597],\n        [-0.96629477],\n        [-0.9197824 ],\n        [-0.59502655],\n        [-0.77848864],\n        [-0.4951984 ],\n        [-0.85492605],\n        [-0.98728675],\n        [-0.88694584],\n        [-0.3749395 ],\n        [-0.96433514],\n        [-0.9794581 ],\n        [-0.9482068 ],\n        [-0.9958253 ],\n        [-0.9505752 ],\n        [-0.93837833],\n        [-0.9555824 ]],\n\n       [[-0.48309934],\n        [-0.9538951 ],\n        [-0.78106093],\n        [-0.992819  ],\n        [-0.90975094],\n        [-0.9874886 ],\n        [-0.7962242 ],\n        [-0.9478574 ],\n        [-0.92408365],\n        [ 0.55123633],\n        [-0.73279035],\n        [-0.9450274 ],\n        [ 0.23214392],\n        [-0.9665703 ],\n        [-0.96381307],\n        [-0.9643145 ],\n        [-0.6010176 ],\n        [ 0.48775274],\n        [-0.954991  ],\n        [-0.91860396],\n        [-0.9325471 ],\n        [-0.96888256],\n        [-0.9528153 ],\n        [-0.98911077],\n        [-0.9711536 ],\n        [-0.7505451 ],\n        [-0.9748467 ],\n        [-0.9531897 ]],\n\n       [[-0.93333477],\n        [-0.99411505],\n        [-0.9894652 ],\n        [-0.97907823],\n        [-0.64584   ],\n        [-0.9922869 ],\n        [-0.81389815],\n        [ 0.7246541 ],\n        [-0.9876018 ],\n        [-0.95606846],\n        [-0.9278572 ],\n        [-0.9168231 ],\n        [-0.80109876],\n        [ 0.47752815],\n        [-0.9557645 ],\n        [-0.9690648 ],\n        [-0.97092086],\n        [-0.98754483],\n        [-0.17508984],\n        [-0.9681277 ],\n        [-0.921866  ],\n        [-0.07156694],\n        [-0.9683019 ],\n        [-0.9694789 ],\n        [-0.9366346 ],\n        [-0.9940248 ],\n        [-0.9701017 ],\n        [-0.9833492 ]],\n\n       [[-0.9945189 ],\n        [-0.9917844 ],\n        [-0.91082263],\n        [-0.9615535 ],\n        [-0.92137367],\n        [-0.9109288 ],\n        [-0.938988  ],\n        [-0.8135015 ],\n        [-0.6901616 ],\n        [-0.9892398 ],\n        [-0.65208614],\n        [ 0.8407475 ],\n        [-0.8654117 ],\n        [-0.9277444 ],\n        [-0.88693935],\n        [-0.73338205],\n        [-0.94919527],\n        [-0.7966144 ],\n        [-0.01483345],\n        [-0.960508  ],\n        [-0.9803806 ],\n        [-0.7944982 ],\n        [-0.975529  ],\n        [-0.995886  ],\n        [-0.995821  ],\n        [-0.8902669 ],\n        [-0.99316436],\n        [-0.99520075]],\n\n       [[-0.9660216 ],\n        [-0.9883837 ],\n        [-0.93716896],\n        [-0.9942154 ],\n        [-0.7782832 ],\n        [-0.15800388],\n        [-0.9519862 ],\n        [-0.9678987 ],\n        [-0.25057507],\n        [-0.88908553],\n        [ 0.9079229 ],\n        [-0.85590386],\n        [-0.60846186],\n        [-0.662265  ],\n        [-0.6921118 ],\n        [-0.68886334],\n        [-0.8892292 ],\n        [-0.9565117 ],\n        [-0.33471712],\n        [-0.9468958 ],\n        [-0.94435245],\n        [-0.36136705],\n        [-0.5547676 ],\n        [-0.92640173],\n        [-0.93914527],\n        [-0.9308812 ],\n        [-0.94139904],\n        [-0.9415698 ]],\n\n       [[-0.97486484],\n        [-0.9685309 ],\n        [-0.48433173],\n        [-0.96718305],\n        [-0.9057479 ],\n        [-0.99234074],\n        [-0.9171208 ],\n        [-0.8878366 ],\n        [-0.6640635 ],\n        [-0.9738369 ],\n        [-0.91799736],\n        [-0.8813939 ],\n        [-0.45062128],\n        [-0.91027915],\n        [ 0.09237592],\n        [ 0.7610556 ],\n        [-0.9181851 ],\n        [-0.65205926],\n        [-0.901434  ],\n        [-0.9688271 ],\n        [-0.90544033],\n        [-0.95256406],\n        [-0.9845024 ],\n        [-0.98013395],\n        [-0.9515254 ],\n        [-0.8211975 ],\n        [-0.9902146 ],\n        [-0.9949052 ]],\n\n       [[-0.99069303],\n        [-0.99459326],\n        [-0.8527023 ],\n        [-0.9854886 ],\n        [-0.96800506],\n        [-0.7792433 ],\n        [-0.9721077 ],\n        [-0.75188863],\n        [ 0.899594  ],\n        [-0.9507427 ],\n        [-0.76585186],\n        [-0.8696525 ],\n        [-0.7541388 ],\n        [-0.11744216],\n        [-0.6590181 ],\n        [ 0.34458333],\n        [-0.92568225],\n        [-0.49354   ],\n        [-0.9205706 ],\n        [-0.54404354],\n        [-0.6352492 ],\n        [-0.9610007 ],\n        [-0.82543385],\n        [-0.99342185],\n        [-0.91996586],\n        [-0.978943  ],\n        [-0.9521903 ],\n        [-0.98789346]],\n\n       [[ 0.72585005],\n        [-0.6033479 ],\n        [-0.9930955 ],\n        [-0.9412533 ],\n        [-0.97097075],\n        [-0.9632623 ],\n        [-0.7442412 ],\n        [ 0.18721223],\n        [-0.97148275],\n        [-0.8934452 ],\n        [-0.6355089 ],\n        [-0.08035783],\n        [-0.92653733],\n        [-0.09970059],\n        [-0.92945707],\n        [-0.4006288 ],\n        [ 0.93982744],\n        [ 0.56456316],\n        [-0.95901275],\n        [-0.20953992],\n        [-0.9381327 ],\n        [-0.95729554],\n        [-0.88481957],\n        [-0.4374805 ],\n        [-0.9779765 ],\n        [-0.9205995 ],\n        [-0.92990947],\n        [-0.9860007 ]],\n\n       [[-0.35780957],\n        [-0.94526064],\n        [-0.99529076],\n        [-0.9384706 ],\n        [-0.9411039 ],\n        [-0.94715446],\n        [-0.9003449 ],\n        [ 0.7271447 ],\n        [-0.7244634 ],\n        [-0.13148808],\n        [-0.9265031 ],\n        [-0.9284355 ],\n        [ 0.34032723],\n        [-0.41091266],\n        [-0.83430123],\n        [-0.74662966],\n        [ 0.21624963],\n        [-0.595734  ],\n        [-0.7147922 ],\n        [-0.70213336],\n        [-0.77996576],\n        [-0.43601587],\n        [-0.9758604 ],\n        [-0.9710674 ],\n        [ 0.13289946],\n        [-0.95302856],\n        [-0.4494425 ],\n        [-0.61932087]],\n\n       [[-0.9310482 ],\n        [-0.9699228 ],\n        [-0.9854179 ],\n        [-0.99086076],\n        [-0.9407996 ],\n        [-0.9678201 ],\n        [-0.98501706],\n        [-0.32619947],\n        [ 0.5610482 ],\n        [ 0.52285534],\n        [-0.6238157 ],\n        [-0.87983733],\n        [ 0.32654577],\n        [-0.9441021 ],\n        [-0.24374324],\n        [ 0.5684813 ],\n        [ 0.56894237],\n        [ 0.41706353],\n        [ 0.681337  ],\n        [-0.9535399 ],\n        [-0.970485  ],\n        [-0.98229736],\n        [-0.96385074],\n        [-0.9959868 ],\n        [-0.97469145],\n        [-0.9890831 ],\n        [-0.9899493 ],\n        [ 0.18440552]],\n\n       [[-0.9267663 ],\n        [-0.71770084],\n        [-0.9887324 ],\n        [-0.9807118 ],\n        [-0.7542782 ],\n        [-0.91307694],\n        [-0.95842576],\n        [-0.44171414],\n        [-0.9207464 ],\n        [-0.7724942 ],\n        [-0.9434611 ],\n        [-0.46998453],\n        [-0.8843158 ],\n        [-0.71387494],\n        [-0.9328243 ],\n        [ 0.43706438],\n        [ 0.7568622 ],\n        [ 0.6244964 ],\n        [ 0.5508931 ],\n        [-0.67139554],\n        [-0.49308676],\n        [-0.908485  ],\n        [-0.98007214],\n        [-0.6998425 ],\n        [-0.90110546],\n        [-0.99622935],\n        [-0.8529024 ],\n        [-0.7750347 ]],\n\n       [[-0.7253765 ],\n        [-0.25895834],\n        [-0.9486557 ],\n        [-0.8639086 ],\n        [-0.9467826 ],\n        [-0.9531291 ],\n        [-0.82383955],\n        [-0.89266723],\n        [-0.4947418 ],\n        [-0.6344807 ],\n        [ 0.9520333 ],\n        [ 0.67839557],\n        [-0.9441449 ],\n        [ 0.73959905],\n        [ 0.77238417],\n        [-0.7520926 ],\n        [ 0.83419645],\n        [-0.21832015],\n        [-0.95306087],\n        [-0.8861122 ],\n        [-0.9070856 ],\n        [-0.84000015],\n        [-0.31743494],\n        [-0.68374044],\n        [-0.9750463 ],\n        [-0.97383475],\n        [-0.92250496],\n        [-0.9822432 ]],\n\n       [[-0.9848232 ],\n        [ 0.6421815 ],\n        [-0.9871709 ],\n        [-0.97540647],\n        [-0.8185446 ],\n        [-0.9664639 ],\n        [ 0.1763174 ],\n        [-0.90363127],\n        [ 0.33940196],\n        [-0.52680945],\n        [ 0.7368061 ],\n        [-0.8225304 ],\n        [-0.78472495],\n        [-0.04879278],\n        [ 0.3810929 ],\n        [ 0.8203086 ],\n        [ 0.8218176 ],\n        [-0.02095024],\n        [-0.8145923 ],\n        [-0.03290599],\n        [ 0.20369588],\n        [ 0.5207648 ],\n        [ 0.48170516],\n        [-0.8649374 ],\n        [-0.94476116],\n        [-0.9619639 ],\n        [-0.7156612 ],\n        [-0.47446743]],\n\n       [[-0.91829693],\n        [-0.9867924 ],\n        [-0.9615701 ],\n        [-0.949935  ],\n        [-0.89609474],\n        [-0.9086386 ],\n        [-0.96943146],\n        [-0.6451456 ],\n        [ 0.1273122 ],\n        [-0.58063596],\n        [ 0.6872564 ],\n        [ 0.70655835],\n        [ 0.6239622 ],\n        [ 0.9276206 ],\n        [ 0.89740306],\n        [ 0.9090309 ],\n        [ 0.6104702 ],\n        [-0.896401  ],\n        [ 0.64532405],\n        [-0.63415045],\n        [-0.89035034],\n        [-0.6926704 ],\n        [-0.8877472 ],\n        [-0.89422524],\n        [-0.9675346 ],\n        [-0.9668996 ],\n        [-0.9886305 ],\n        [-0.89449894]],\n\n       [[-0.98196566],\n        [-0.9917392 ],\n        [-0.9754122 ],\n        [-0.943113  ],\n        [-0.989177  ],\n        [-0.97826105],\n        [-0.7666992 ],\n        [-0.70748067],\n        [-0.89161175],\n        [-0.911669  ],\n        [-0.5202202 ],\n        [ 0.9131763 ],\n        [ 0.8165383 ],\n        [-0.8020625 ],\n        [ 0.5572742 ],\n        [-0.02810982],\n        [-0.32226175],\n        [-0.80499   ],\n        [ 0.33295   ],\n        [-0.95888215],\n        [-0.9311479 ],\n        [-0.9541638 ],\n        [-0.900386  ],\n        [-0.96604276],\n        [-0.79680216],\n        [-0.9700831 ],\n        [-0.8411364 ],\n        [-0.9821916 ]],\n\n       [[-0.9582323 ],\n        [-0.9862565 ],\n        [-0.9088534 ],\n        [-0.9042095 ],\n        [-0.73364013],\n        [-0.48402798],\n        [-0.9455297 ],\n        [-0.98491776],\n        [-0.9694263 ],\n        [-0.60691154],\n        [ 0.7934336 ],\n        [-0.85235834],\n        [ 0.7556982 ],\n        [-0.02353771],\n        [ 0.4228999 ],\n        [-0.21393052],\n        [-0.9110839 ],\n        [-0.7353219 ],\n        [-0.983423  ],\n        [-0.9079171 ],\n        [-0.9710031 ],\n        [-0.34735647],\n        [-0.8647309 ],\n        [-0.99340254],\n        [-0.92115676],\n        [-0.84959835],\n        [-0.9220667 ],\n        [-0.9489017 ]],\n\n       [[-0.9258114 ],\n        [-0.9051101 ],\n        [-0.8882668 ],\n        [-0.9453425 ],\n        [-0.83675474],\n        [-0.98893917],\n        [-0.8635158 ],\n        [-0.45849884],\n        [-0.6030546 ],\n        [-0.89078903],\n        [ 0.75370026],\n        [-0.41899937],\n        [-0.03217761],\n        [ 0.11120733],\n        [-0.7664208 ],\n        [-0.03989077],\n        [-0.8905667 ],\n        [-0.8990559 ],\n        [-0.02405145],\n        [-0.90718013],\n        [-0.96873176],\n        [-0.96437097],\n        [-0.8397684 ],\n        [-0.8283772 ],\n        [-0.6563902 ],\n        [-0.9823853 ],\n        [-0.91801333],\n        [-0.9435345 ]],\n\n       [[-0.9505327 ],\n        [-0.9115125 ],\n        [-0.84032845],\n        [-0.94272786],\n        [-0.98695254],\n        [-0.96287507],\n        [-0.9320673 ],\n        [-0.9456122 ],\n        [-0.9563936 ],\n        [-0.8747542 ],\n        [-0.9402374 ],\n        [-0.8377955 ],\n        [-0.8928328 ],\n        [ 0.17696565],\n        [ 0.40989295],\n        [ 0.07835376],\n        [-0.9382222 ],\n        [-0.95838773],\n        [ 0.36493367],\n        [-0.6604326 ],\n        [-0.7662208 ],\n        [-0.7728796 ],\n        [-0.7606169 ],\n        [-0.9826058 ],\n        [ 0.81440234],\n        [-0.8848137 ],\n        [-0.9281346 ],\n        [-0.99313575]],\n\n       [[-0.99315417],\n        [-0.93259937],\n        [-0.9899653 ],\n        [-0.9825403 ],\n        [-0.99235415],\n        [-0.9230143 ],\n        [-0.98148966],\n        [-0.9577293 ],\n        [-0.91864705],\n        [-0.953135  ],\n        [-0.76959056],\n        [-0.9008835 ],\n        [-0.90384424],\n        [-0.87029076],\n        [-0.6533354 ],\n        [-0.9032253 ],\n        [-0.8071469 ],\n        [-0.736848  ],\n        [-0.9355507 ],\n        [-0.5958418 ],\n        [-0.7944747 ],\n        [-0.7367949 ],\n        [-0.9225209 ],\n        [-0.97088367],\n        [ 0.31575006],\n        [ 0.65870535],\n        [-0.97649974],\n        [-0.969149  ]],\n\n       [[-0.9688603 ],\n        [-0.95320916],\n        [-0.9908652 ],\n        [-0.96646065],\n        [-0.36531493],\n        [-0.9454932 ],\n        [-0.93973094],\n        [-0.9914796 ],\n        [-0.94877326],\n        [-0.65276855],\n        [-0.9838989 ],\n        [-0.02410929],\n        [ 0.07218436],\n        [-0.12982266],\n        [-0.8425138 ],\n        [-0.9354643 ],\n        [-0.3547802 ],\n        [-0.7551189 ],\n        [-0.98829085],\n        [-0.77896225],\n        [-0.8337279 ],\n        [-0.96702915],\n        [-0.98612845],\n        [-0.86950505],\n        [-0.89058185],\n        [-0.9943438 ],\n        [-0.54350466],\n        [-0.5632836 ]],\n\n       [[-0.8590307 ],\n        [ 0.79445195],\n        [-0.9693568 ],\n        [-0.9540237 ],\n        [-0.9827905 ],\n        [-0.97870916],\n        [-0.98163664],\n        [-0.9801267 ],\n        [-0.47017348],\n        [-0.7209612 ],\n        [-0.94878465],\n        [-0.950856  ],\n        [-0.7630877 ],\n        [-0.88914514],\n        [-0.6911677 ],\n        [-0.9256208 ],\n        [-0.7554029 ],\n        [-0.8834267 ],\n        [ 0.59620637],\n        [-0.8853617 ],\n        [-0.67136574],\n        [-0.97143424],\n        [-0.8327178 ],\n        [-0.9768985 ],\n        [-0.9946627 ],\n        [-0.51238763],\n        [-0.94501495],\n        [-0.4662969 ]],\n\n       [[ 0.38188016],\n        [-0.9883743 ],\n        [-0.9478392 ],\n        [-0.9724679 ],\n        [-0.51537746],\n        [-0.8224024 ],\n        [-0.98763317],\n        [-0.9610016 ],\n        [-0.5931149 ],\n        [-0.9881883 ],\n        [ 0.59005696],\n        [-0.9853975 ],\n        [-0.96105766],\n        [-0.87230074],\n        [-0.93856084],\n        [-0.9838631 ],\n        [-0.8516947 ],\n        [-0.97683203],\n        [-0.96216094],\n        [-0.27113163],\n        [-0.96823454],\n        [-0.9451272 ],\n        [ 0.47410306],\n        [ 0.452332  ],\n        [-0.8169567 ],\n        [-0.829307  ],\n        [-0.9725292 ],\n        [-0.95597285]],\n\n       [[-0.98669636],\n        [-0.8225887 ],\n        [-0.8794188 ],\n        [ 0.5097075 ],\n        [-0.7949002 ],\n        [-0.9871981 ],\n        [-0.6016341 ],\n        [-0.93451655],\n        [-0.6664932 ],\n        [ 0.5313722 ],\n        [-0.9746527 ],\n        [-0.95626384],\n        [-0.94526726],\n        [-0.9545435 ],\n        [-0.98564845],\n        [ 0.36529368],\n        [-0.99051887],\n        [-0.9626631 ],\n        [-0.9948199 ],\n        [-0.9838029 ],\n        [-0.935259  ],\n        [-0.9122395 ],\n        [-0.9272727 ],\n        [-0.9517331 ],\n        [-0.89005286],\n        [-0.9831779 ],\n        [-0.899185  ],\n        [-0.9721307 ]],\n\n       [[-0.9717943 ],\n        [-0.96353745],\n        [-0.9629532 ],\n        [-0.97219175],\n        [-0.89715093],\n        [-0.9801774 ],\n        [-0.96094054],\n        [-0.79889363],\n        [-0.48891523],\n        [-0.91307473],\n        [ 0.10274511],\n        [-0.86262   ],\n        [ 0.57985365],\n        [-0.98110086],\n        [-0.9690898 ],\n        [-0.8370893 ],\n        [-0.944151  ],\n        [-0.9479217 ],\n        [-0.9078    ],\n        [-0.9587604 ],\n        [-0.96552527],\n        [-0.97499394],\n        [-0.97402006],\n        [-0.94386125],\n        [-0.93041444],\n        [-0.98013026],\n        [-0.9628975 ],\n        [-0.99788743]],\n\n       [[-0.9564357 ],\n        [-0.5844874 ],\n        [-0.30569768],\n        [-0.8816243 ],\n        [-0.96140176],\n        [-0.9555432 ],\n        [-0.94162124],\n        [-0.6796513 ],\n        [-0.9809129 ],\n        [-0.9538626 ],\n        [-0.9042666 ],\n        [-0.90156436],\n        [-0.7754948 ],\n        [-0.91014403],\n        [-0.9271704 ],\n        [-0.93592584],\n        [-0.9843839 ],\n        [-0.9340135 ],\n        [-0.96032196],\n        [-0.9835339 ],\n        [-0.82509005],\n        [-0.7269048 ],\n        [-0.9428511 ],\n        [-0.97755736],\n        [-0.95746875],\n        [-0.9707339 ],\n        [-0.9354424 ],\n        [-0.9536865 ]]], dtype=float32)"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model((torch.rand(1, 128) - 0.5) / 0.5)[0].detach().numpy().transpose((1, 2, 0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T14:28:50.267901500Z",
     "start_time": "2024-01-31T14:28:50.245881100Z"
    }
   },
   "id": "4aeb9931a3e778a8",
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.5395,  0.9544,  0.6611, -0.4757, -0.8301,  0.8045,  0.3216,  0.9179,\n         -0.6428,  0.1987,  0.1345,  0.5613, -0.2579,  0.2598,  0.3434, -0.4156,\n         -0.6296,  0.0082, -0.6542,  0.1299,  0.3490, -0.7853, -0.0239, -0.6955,\n         -0.8505, -0.9913,  0.8528, -0.1722, -0.3267, -0.1016, -0.2829,  0.4351,\n          0.9579, -0.3798,  0.1456,  0.4221,  0.9750, -0.0136,  0.1775, -0.5544,\n         -0.6090,  0.6258,  0.9881, -0.4932, -0.5739,  0.8516,  0.5848,  0.6350,\n         -0.9634,  0.8150,  0.2418, -0.5369,  0.7791, -0.2261, -0.3193, -0.3820,\n         -0.6224, -0.9535, -0.5765, -0.4264,  0.3109,  0.5459, -0.7324, -0.7304,\n          0.8035, -0.5826, -0.6420,  0.7753,  0.5434,  0.6501, -0.6498, -0.3730,\n          0.1997, -0.7406,  0.6460, -0.7222,  0.5282,  0.5178,  0.3215,  0.5724,\n         -0.8679,  0.3225,  0.3004, -0.0300, -0.7594, -0.2622, -0.5610,  0.2544,\n         -0.7090, -0.7260,  0.4633, -0.8133,  0.5436,  0.2079,  0.9787,  0.4213,\n          0.4528,  0.3204,  0.1299, -0.9573,  0.9967,  0.3368,  0.9262, -0.7252,\n         -0.2299, -0.6507, -0.5733, -0.5686, -0.3271,  0.1590, -0.2812,  0.2747,\n         -0.1792, -0.1930, -0.4264,  0.9642,  0.1258, -0.1519, -0.3710, -0.1791,\n         -0.6297, -0.2863,  0.8949,  0.5872, -0.0263,  0.2394, -0.7118, -0.7325]])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T14:22:55.104340700Z",
     "start_time": "2024-01-31T14:22:55.084511900Z"
    }
   },
   "id": "3e336777636fbf66",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.3784, 0.8643, 0.2690,  ..., 0.4714, 0.5403, 0.0324],\n        [0.9346, 0.8003, 0.8330,  ..., 0.2683, 0.6323, 0.8304],\n        [0.5615, 0.0870, 0.0309,  ..., 0.0019, 0.1108, 0.0951],\n        ...,\n        [0.1683, 0.0143, 0.8850,  ..., 0.4500, 0.0877, 0.6817],\n        [0.7285, 0.1778, 0.6184,  ..., 0.5382, 0.1856, 0.6224],\n        [0.9671, 0.6437, 0.0507,  ..., 0.0755, 0.1245, 0.8288]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(real_inputs.shape[0], 128)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T18:11:52.577133300Z",
     "start_time": "2024-01-29T18:11:52.568923300Z"
    }
   },
   "id": "a81758dd7e75e529",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.3078, -0.2757, -0.6496,  ...,  0.5296, -0.9710, -0.1025],\n        [-0.4933, -0.1092, -0.4971,  ..., -0.4730,  0.9784, -0.5342],\n        [ 0.1277,  0.2297,  0.0028,  ...,  0.0484,  0.7489,  0.5370],\n        ...,\n        [-0.2825,  0.4791, -0.6772,  ..., -0.6195, -0.6709,  0.2323],\n        [ 0.1807, -0.8672,  0.5920,  ...,  0.9913,  0.1850,  0.7826],\n        [-0.7598, -0.0346, -0.0097,  ..., -0.1969, -0.5449,  0.1601]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " (torch.rand(real_inputs.shape[0], 128) - 0.5) / 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-29T18:12:05.271299Z",
     "start_time": "2024-01-29T18:12:05.264259800Z"
    }
   },
   "id": "fb00993c7386696d",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5cb3d1ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T10:09:44.910989900Z",
     "start_time": "2024-01-15T10:08:13.127356500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | generator     | Generator     | 1.5 M \n",
      "1 | discriminator | Discriminator | 1.5 M \n",
      "------------------------------------------------\n",
      "2.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 M     Total params\n",
      "11.786    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85e0075f6a55420f8002ee09b7fedda6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x0000023C6465E700> (for post_execute):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class GAN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # print(1)\n",
    "        self.generator = Generator()\n",
    "        self.discriminator = Discriminator()\n",
    "        # After each epoch, we generate 100 images using the noise\n",
    "        # vector here (self.test_noises). We save the output images\n",
    "        # in a list (self.test_progression) for plotting later.\n",
    "        self.test_noises = torch.randn(100, 1, 100, device=device)\n",
    "        self.test_progression = []\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Generates an image using the generator\n",
    "        given input noise z\n",
    "        \"\"\"\n",
    "        #print('2')\n",
    "        return self.generator(z)\n",
    "\n",
    "    def generator_step(self, x):\n",
    "        \"\"\"\n",
    "        Training step for generator\n",
    "        1. Sample random noise\n",
    "        2. Pass noise to generator to\n",
    "           generate images\n",
    "        3. Classify generated images using\n",
    "           the discriminator\n",
    "        4. Backprop loss to the generator\n",
    "        \"\"\"\n",
    "        #print('3')\n",
    "\n",
    "        # Sample noise\n",
    "        z = torch.randn(x.shape[0], 1, 100, device=device)\n",
    "\n",
    "        # Generate images\n",
    "        generated_imgs = self(z)\n",
    "\n",
    "        # Classify generated images\n",
    "        # using the discriminator\n",
    "        d_output = torch.squeeze(self.discriminator(generated_imgs))\n",
    "\n",
    "        # Backprop loss. We want to maximize the discriminator's\n",
    "        # loss, which is equivalent to minimizing the loss with the true\n",
    "        # labels flipped (i.e. y_true=1 for fake images). We do this\n",
    "        # as PyTorch can only minimize a function instead of maximizing\n",
    "        g_loss = nn.BCELoss()(d_output,\n",
    "                              torch.ones(x.shape[0], device=device))\n",
    "\n",
    "        return g_loss\n",
    "\n",
    "    def discriminator_step(self, x):\n",
    "        #print('4')\n",
    "        \"\"\"\n",
    "        Training step for discriminator\n",
    "        1. Get actual images\n",
    "        2. Predict probabilities of actual images and get BCE loss\n",
    "        3. Get fake images from generator\n",
    "        4. Predict probabilities of fake images and get BCE loss\n",
    "        5. Combine loss from both and backprop loss to discriminator\n",
    "        \"\"\"\n",
    "\n",
    "        # Real images\n",
    "        d_output = torch.squeeze(self.discriminator(x))\n",
    "        loss_real = nn.BCELoss()(d_output,\n",
    "                                 torch.ones(x.shape[0], device=device))\n",
    "\n",
    "        # Fake images\n",
    "        z = torch.randn(x.shape[0], 1, 100, device=device)\n",
    "        generated_imgs = self(z)\n",
    "        d_output = torch.squeeze(self.discriminator(generated_imgs))\n",
    "        loss_fake = nn.BCELoss()(d_output,\n",
    "                                 torch.zeros(x.shape[0], device=device))\n",
    "\n",
    "        return loss_real + loss_fake\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #print('5')\n",
    "        X, _ = batch\n",
    "        plt.imshow(np.reshape(X[0].detach().numpy(), (28, 28)))\n",
    "        g_opt, d_opt = self.optimizers()\n",
    "\n",
    "        g_loss = self.generator_step(X)\n",
    "        g_opt.zero_grad()\n",
    "        self.manual_backward(g_loss)\n",
    "        g_opt.step()\n",
    "\n",
    "        d_loss = self.discriminator_step(X)\n",
    "        d_opt.zero_grad()\n",
    "        self.manual_backward(d_loss)\n",
    "        d_opt.step()\n",
    "\n",
    "        #print(g_loss, d_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #print('6')\n",
    "        g_opt = torch.optim.Adam(self.generator.parameters(), lr=0.001)\n",
    "        d_opt = torch.optim.Adam(self.discriminator.parameters(), lr=0.001)\n",
    "        return g_opt, d_opt\n",
    "\n",
    "    def on_train_epoch_end(self):  # , training_step_outputs\n",
    "        #print('7')\n",
    "        epoch_test_images = self(self.test_noises)\n",
    "        self.test_progression.append(epoch_test_images)\n",
    "\n",
    "\n",
    "#Теперь мы можем обучить наш GAN. Мы будем обучать его с помощью графического процессора в течение 100 эпох.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GAN()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=2)  #, max_epochs=40,accelerator=\"auto\"\n",
    "trainer.fit(model, mnist_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01d530e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T07:58:56.309958600Z",
     "start_time": "2024-01-15T07:58:56.289682700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8d691ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-15T10:09:50.853607400Z",
     "start_time": "2024-01-15T10:09:50.788963100Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[114], line 25\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(ncol):\n\u001B[0;32m     24\u001B[0m     idx \u001B[38;5;241m=\u001B[39m i\u001B[38;5;241m*\u001B[39mncol \u001B[38;5;241m+\u001B[39m j\n\u001B[1;32m---> 25\u001B[0m     img \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mreshape(images[epoch_to_plot\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][indexes[idx]], (\u001B[38;5;241m28\u001B[39m,\u001B[38;5;241m28\u001B[39m))\n\u001B[0;32m     26\u001B[0m     ax \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplot(gs[i,j])\n\u001B[0;32m     27\u001B[0m     ax\u001B[38;5;241m.\u001B[39mimshow(img, cmap\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgray\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1800x1000 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt, gridspec\n",
    "\n",
    "# Convert images from torch tensor to numpy array\n",
    "images = [i.detach().cpu().numpy() for i in model.test_progression]\n",
    "\n",
    "epoch_to_plot = 1\n",
    "nrow = 4\n",
    "ncol = 8\n",
    "\n",
    "# randomly select 10 images for plotting\n",
    "indexes = np.random.choice(range(100), nrow * ncol, replace=False)\n",
    "\n",
    "fig = plt.figure(figsize=((ncol + 1) * 2, (nrow + 1) * 2))\n",
    "fig.suptitle('Epoch {}'.format(epoch_to_plot), fontsize=30)\n",
    "\n",
    "gs = gridspec.GridSpec(nrow, ncol,\n",
    "                       wspace=0.0, hspace=0.0,\n",
    "                       top=1. - 0.5 / (nrow + 1), bottom=0.5 / (nrow + 1),\n",
    "                       left=0.5 / (ncol + 1), right=1 - 0.5 / (ncol + 1))\n",
    "\n",
    "for i in range(nrow):\n",
    "    for j in range(ncol):\n",
    "        idx = i * ncol + j\n",
    "        img = np.reshape(images[epoch_to_plot - 1][indexes[idx]], (28, 28))\n",
    "        ax = plt.subplot(gs[i, j])\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.        , -1.        , -1.        , -1.        ,  1.        ,\n         1.        , -1.        , -1.        , -1.        , -1.        ,\n         1.        , -1.        , -1.        ,  1.        ,  1.        ,\n         1.        ,  1.        , -1.        , -1.        , -1.        ,\n         1.        , -1.        , -1.        , -1.        ,  1.        ,\n        -1.        , -1.        , -1.        ],\n       [-1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        , -1.        ,\n         1.        ,  1.        ,  1.        , -1.        ,  1.        ,\n        -1.        ,  1.        ,  1.        , -1.        , -1.        ,\n         1.        ,  1.        , -1.        ,  1.        , -1.        ,\n        -1.        ,  1.        , -1.        ],\n       [-1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        ,  1.        ,  1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        ,  1.        ,\n         1.        ,  1.        , -1.        ,  1.        , -1.        ,\n        -1.        ,  1.        , -1.        ],\n       [-1.        ,  1.        , -1.        ,  1.        , -1.        ,\n        -1.        ,  1.        , -1.        ,  1.        ,  1.        ,\n        -1.        ,  1.        , -1.        , -1.        , -1.        ,\n        -1.        ,  1.        , -1.        ,  1.        , -1.        ,\n         1.        ,  1.        ,  0.99998266, -1.        , -1.        ,\n        -1.        , -1.        ,  1.        ],\n       [ 1.        ,  1.        ,  1.        , -1.        , -1.        ,\n        -1.        ,  1.        ,  1.        , -1.        , -1.        ,\n         1.        , -1.        , -1.        ,  1.        ,  1.        ,\n        -1.        ,  1.        , -1.        , -1.        , -1.        ,\n        -1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n        -1.        , -1.        , -1.        ],\n       [-1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        ,  1.        , -1.        ,  1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        ,  1.        ,\n        -0.9999999 , -1.        ,  1.        , -1.        , -1.        ,\n        -1.        , -1.        ,  1.        , -1.        , -1.        ,\n         1.        ,  1.        ,  1.        ],\n       [-1.        ,  1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        ,  1.        , -1.        , -1.        , -1.        ,\n         1.        , -1.        , -1.        ,  1.        ,  1.        ,\n        -1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        ,  1.        , -1.        ],\n       [-1.        , -1.        , -1.        , -1.        ,  1.        ,\n        -1.        ,  1.        , -1.        , -1.        ,  1.        ,\n         1.        ,  1.        ,  1.        ,  1.        , -1.        ,\n         1.        ,  1.        , -1.        ,  1.        ,  1.        ,\n        -1.        ,  1.        , -1.        ,  1.        , -1.        ,\n        -1.        , -1.        ,  1.        ],\n       [-1.        , -1.        , -1.        , -1.        ,  1.        ,\n        -1.        , -1.        , -1.        , -1.        ,  1.        ,\n         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n         1.        ,  1.        ,  1.        , -1.        , -1.        ,\n         1.        , -1.        , -1.        ,  1.        , -1.        ,\n        -1.        , -1.        , -1.        ],\n       [-1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        ,  1.        ,\n        -1.        , -1.        , -0.9999998 ,  1.        , -1.        ,\n        -0.9999146 ,  1.        ,  1.        ,  1.        , -1.        ,\n        -1.        ,  1.        ,  1.        , -1.        , -1.        ,\n        -1.        ,  1.        , -1.        ],\n       [-1.        , -1.        , -1.        ,  0.99999994, -1.        ,\n        -1.        ,  1.        ,  1.        , -1.        , -1.        ,\n        -1.        , -1.        ,  1.        , -1.        ,  1.        ,\n        -1.        , -1.        , -1.        ,  1.        ,  1.        ,\n        -1.        , -1.        , -1.        ,  1.        ,  1.        ,\n         1.        , -1.        , -1.        ],\n       [ 1.        , -1.        , -1.        , -1.        , -1.        ,\n         1.        ,  1.        ,  0.99995935, -1.        ,  1.        ,\n        -1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n         1.        ,  1.        ,  1.        ,  1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        ,  1.        ],\n       [-1.        ,  1.        ,  1.        , -1.        , -1.        ,\n        -1.        , -1.        ,  1.        ,  1.        ,  1.        ,\n        -1.        , -0.9999998 , -1.        ,  1.        ,  1.        ,\n         1.        ,  1.        ,  1.        , -1.        ,  1.        ,\n        -1.        ,  1.        , -1.        , -1.        , -1.        ,\n        -1.        ,  1.        , -1.        ],\n       [-1.        ,  1.        ,  1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        ,  1.        ,\n        -1.        ,  1.        , -1.        , -1.        ,  1.        ,\n         1.        , -1.        ,  1.        , -1.        ,  1.        ,\n        -1.        ,  1.        , -1.        , -1.        , -1.        ,\n         1.        , -1.        ,  1.        ],\n       [-1.        , -1.        ,  1.        , -1.        , -1.        ,\n         1.        ,  1.        , -1.        ,  1.        ,  1.        ,\n         1.        , -1.        , -1.        ,  1.        ,  1.        ,\n         1.        , -1.        , -1.        ,  1.        ,  1.        ,\n        -1.        ,  1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        ],\n       [-1.        ,  1.        , -1.        , -1.        ,  1.        ,\n         1.        , -1.        ,  1.        ,  1.        ,  1.        ,\n        -0.99999994, -1.        , -1.        ,  1.        , -1.        ,\n        -1.        , -1.        ,  1.        ,  1.        ,  1.        ,\n         1.        , -1.        ,  1.        , -1.        , -1.        ,\n         1.        , -1.        , -1.        ],\n       [-1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        ,  1.        , -1.        ,  1.        , -1.        ,\n         1.        ,  1.        ,  1.        , -1.        ,  1.        ,\n         1.        ,  1.        , -1.        , -1.        ,  1.        ,\n         1.        ,  1.        ,  1.        , -1.        ,  1.        ,\n        -1.        , -1.        ,  1.        ],\n       [-1.        ,  1.        , -1.        , -1.        , -1.        ,\n         1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        ,  1.        , -1.        ,  1.        ,  1.        ,\n         1.        , -1.        ,  1.        ,  1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        ,  1.        ],\n       [-1.        , -1.        , -1.        ,  1.        , -1.        ,\n        -1.        ,  1.        , -1.        , -1.        , -0.9999994 ,\n        -1.        , -1.        ,  1.        ,  1.        , -1.        ,\n         1.        , -1.        ,  1.        , -1.        , -1.        ,\n         1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        ,  1.        ],\n       [-1.        , -1.        ,  1.        , -1.        , -1.        ,\n        -1.        ,  1.        , -1.        ,  1.        , -1.        ,\n        -1.        , -1.        , -1.        ,  1.        , -1.        ,\n         1.        , -1.        ,  1.        , -1.        ,  1.        ,\n        -1.        ,  1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        ],\n       [-1.        , -1.        ,  1.        ,  1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        ,  1.        ,\n        -1.        , -1.        , -1.        , -1.        ,  1.        ,\n         1.        , -1.        , -1.        ,  1.        ,  1.        ,\n        -1.        , -1.        ,  1.        ],\n       [ 1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        ,  1.        ,\n         1.        , -1.        ,  1.        , -1.        ,  1.        ,\n        -1.        , -1.        ,  1.        ,  1.        , -1.        ,\n        -1.        ,  1.        , -1.        , -1.        ,  1.        ,\n         1.        , -1.        ,  1.        ],\n       [ 1.        ,  1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        ,  1.        , -1.        ,\n         1.        , -1.        , -1.        ,  1.        , -1.        ,\n        -1.        ,  1.        ,  1.        ,  1.        , -1.        ,\n         1.        ,  1.        , -1.        ,  1.        , -1.        ,\n        -1.        ,  1.        ,  1.        ],\n       [-1.        ,  1.        ,  1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        ,  1.        ,\n        -1.        ,  1.        , -1.        ,  1.        , -1.        ,\n        -1.        , -1.        , -1.        ,  1.        , -1.        ,\n         1.        ,  1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        ],\n       [-1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        ,  1.        , -1.        ,  1.        , -1.        ,\n        -1.        ,  1.        ,  1.        , -1.        ,  1.        ,\n        -1.        , -1.        , -1.        ,  1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        ],\n       [ 1.        ,  1.        ,  1.        , -1.        , -1.        ,\n         1.        , -1.        , -1.        , -1.        , -0.99999994,\n        -1.        , -1.        , -1.        ,  1.        , -1.        ,\n         1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        ,  1.        , -1.        , -1.        ,\n         1.        ,  1.        , -1.        ],\n       [-1.        , -1.        , -1.        , -1.        ,  1.        ,\n         1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        ,  1.        ],\n       [ 1.        , -1.        , -1.        ,  1.        , -1.        ,\n         1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        ,  1.        ,  1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        , -1.        , -1.        ,\n        -1.        , -1.        , -1.        ,  1.        ,  1.        ,\n        -1.        , -1.        , -1.        ]], dtype=float32)"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(model.forward(torch.randn(1, 1, 100)).detach().numpy(), (28, 28))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T10:09:52.711780200Z",
     "start_time": "2024-01-15T10:09:52.667690500Z"
    }
   },
   "id": "bcb22f8d632737f3"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7cce98c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T06:43:27.282859800Z",
     "start_time": "2024-01-16T06:43:27.260212900Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[57], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m img \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mreshape(model\u001B[38;5;241m.\u001B[39mforward(torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m100\u001B[39m))\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy(),(\u001B[38;5;241m28\u001B[39m,\u001B[38;5;241m28\u001B[39m))\n\u001B[0;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow(img)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "img = np.reshape(model.forward(torch.randn(1, 1, 100)).detach().numpy(), (28, 28))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[ 5.07983446e-01, -8.56134772e-01,  6.58681750e-01,\n         -5.56978881e-01,  4.59508568e-01, -1.33551195e-01,\n          1.58638585e+00, -3.67085814e-01,  7.08193839e-01,\n          2.05746603e+00,  1.57912719e+00,  3.59650582e-01,\n         -1.55611351e-01, -1.28941405e+00,  8.13412786e-01,\n          3.04984629e-01,  1.17076226e-01, -7.31400728e-01,\n          7.24779218e-02, -2.31657958e+00, -7.45728314e-01,\n         -2.80001462e-01,  8.60050857e-01, -8.50371718e-01,\n          1.25235513e-01,  1.00741494e+00, -2.04857901e-01,\n         -1.15286922e+00,  4.12721694e-01,  1.02843940e+00,\n          4.00168508e-01,  6.34475276e-02,  2.54313052e-01,\n          1.00115073e+00, -8.31082985e-02,  1.27650928e+00,\n          4.12714541e-01, -9.92530659e-02, -1.68523073e+00,\n          2.28361294e-01,  2.34285280e-01,  1.47009119e-01,\n          1.43849182e+00, -1.52908236e-01,  3.79862100e-01,\n         -9.10960436e-01, -9.12463605e-01, -1.11210018e-01,\n         -2.93070078e-01, -3.76423985e-01, -9.69222367e-01,\n          1.49738312e+00,  1.78706503e+00, -8.69702041e-01,\n         -3.68321300e-01, -1.10425329e+00,  1.80981231e+00,\n          1.63566530e+00,  1.76322043e+00,  2.18460023e-01,\n          5.95129251e-01, -4.03650939e-01,  1.65122449e-01,\n          1.15395439e+00, -3.83286923e-01, -1.06520104e+00,\n         -5.86800814e-01,  9.89067554e-01,  2.66157597e-01,\n          4.27363589e-02,  4.66425449e-01,  2.82977067e-04,\n          1.83238924e+00, -7.36143947e-01,  2.22287631e+00,\n          3.43196779e-01,  1.99865755e-02,  2.16169715e+00,\n         -2.08142114e+00,  2.19057846e+00, -5.39361358e-01,\n         -2.57332355e-01, -4.95478123e-01,  9.14765358e-01,\n          9.28966761e-01,  1.35699645e-01,  8.36509287e-01,\n          1.39412925e-01, -1.28176057e+00,  7.12661445e-01,\n         -2.12806487e+00, -1.56371631e-02, -1.24901450e+00,\n         -7.95678854e-01,  2.43408394e+00,  9.05388780e-03,\n          1.35305214e+00,  1.06493044e+00,  5.59496164e-01,\n          7.35745668e-01]]], dtype=float32)"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, 1, 100).detach().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T10:10:15.974156600Z",
     "start_time": "2024-01-15T10:10:15.957276Z"
    }
   },
   "id": "1962714bc97b8f40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c01ff",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T06:49:18.299530500Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "mnist_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=0.5, std=0.5),\n",
    "                                       transforms.Lambda(lambda x: x.view(-1, 784))])\n",
    "\n",
    "data = datasets.MNIST(root='/data/MNIST', download=True, transform=mnist_transforms)\n",
    "\n",
    "mnist_dataloader = DataLoader(data, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator class. Accepts a tensor of size 100 as input as outputs another\n",
    "    tensor of size 784. Objective is to generate an output tensor that is\n",
    "    indistinguishable from the real MNIST digits \n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(in_features=100, out_features=256),\n",
    "                                    nn.LeakyReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Linear(in_features=256, out_features=512),\n",
    "                                    nn.LeakyReLU())\n",
    "        self.layer3 = nn.Sequential(nn.Linear(in_features=512, out_features=1024),\n",
    "                                    nn.LeakyReLU())\n",
    "        self.output = nn.Sequential(nn.Linear(in_features=1024, out_features=28 * 28),\n",
    "                                    nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator class. Accepts a tensor of size 784 as input and outputs\n",
    "    a tensor of size 1 as  the predicted class probabilities\n",
    "    (generated or real data)\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(in_features=28 * 28, out_features=1024),\n",
    "                                    nn.LeakyReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Linear(in_features=1024, out_features=512),\n",
    "                                    nn.LeakyReLU())\n",
    "        self.layer3 = nn.Sequential(nn.Linear(in_features=512, out_features=256),\n",
    "                                    nn.LeakyReLU())\n",
    "        self.output = nn.Sequential(nn.Linear(in_features=256, out_features=1),\n",
    "                                    nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853160ee",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T06:49:18.315160300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class GAN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generator = Generator()\n",
    "        self.discriminator = Discriminator()\n",
    "        # After each epoch, we generate 100 images using the noise\n",
    "        # vector here (self.test_noises). We save the output images\n",
    "        # in a list (self.test_progression) for plotting later.\n",
    "        self.test_noises = torch.randn(100, 1, 100, device=device)\n",
    "        self.test_progression = []\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Generates an image using the generator\n",
    "        given input noise z\n",
    "        \"\"\"\n",
    "        return self.generator(z)\n",
    "\n",
    "    def generator_step(self, x):\n",
    "        \"\"\"\n",
    "        Training step for generator\n",
    "        1. Sample random noise\n",
    "        2. Pass noise to generator to\n",
    "           generate images\n",
    "        3. Classify generated images using\n",
    "           the discriminator\n",
    "        4. Backprop loss to the generator\n",
    "        \"\"\"\n",
    "\n",
    "        # Sample noise\n",
    "        z = torch.randn(x.shape[0], 1, 100, device=device)\n",
    "\n",
    "        # Generate images\n",
    "        generated_imgs = self(z)\n",
    "\n",
    "        # Classify generated images\n",
    "        # using the discriminator\n",
    "        d_output = torch.squeeze(self.discriminator(generated_imgs))\n",
    "\n",
    "        # Backprop loss. We want to maximize the discriminator's\n",
    "        # loss, which is equivalent to minimizing the loss with the true\n",
    "        # labels flipped (i.e. y_true=1 for fake images). We do this\n",
    "        # as PyTorch can only minimize a function instead of maximizing\n",
    "        g_loss = nn.BCELoss()(d_output,\n",
    "                              torch.ones(x.shape[0], device=device))\n",
    "\n",
    "        return g_loss\n",
    "\n",
    "    def discriminator_step(self, x):\n",
    "        \"\"\"\n",
    "        Training step for discriminator\n",
    "        1. Get actual images\n",
    "        2. Predict probabilities of actual images and get BCE loss\n",
    "        3. Get fake images from generator\n",
    "        4. Predict probabilities of fake images and get BCE loss\n",
    "        5. Combine loss from both and backprop loss to discriminator\n",
    "        \"\"\"\n",
    "\n",
    "        # Real images\n",
    "        d_output = torch.squeeze(self.discriminator(x))\n",
    "        loss_real = nn.BCELoss()(d_output,\n",
    "                                 torch.ones(x.shape[0], device=device))\n",
    "\n",
    "        # Fake images\n",
    "        z = torch.randn(x.shape[0], 1, 100, device=device)\n",
    "        generated_imgs = self(z)\n",
    "        d_output = torch.squeeze(self.discriminator(generated_imgs))\n",
    "        loss_fake = nn.BCELoss()(d_output,\n",
    "                                 torch.zeros(x.shape[0], device=device))\n",
    "\n",
    "        return loss_real + loss_fake\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        X, _ = batch\n",
    "\n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "            loss = self.generator_step(X)\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:\n",
    "            loss = self.discriminator_step(X)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.0002)\n",
    "        d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.0002)\n",
    "        return [g_optimizer, d_optimizer], []\n",
    "\n",
    "    def on_train_epoch_end(self, training_step_outputs):\n",
    "        epoch_test_images = self(self.test_noises)\n",
    "        self.test_progression.append(epoch_test_images)\n",
    "\n",
    "\n",
    "#Теперь мы можем обучить наш GAN. Мы будем обучать его с помощью графического процессора в течение 100 эпох.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GAN()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=100)\n",
    "trainer.fit(model, mnist_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82d5f8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T06:49:18.317591500Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in torch.manual_seed(42):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a042bf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-15T06:49:18.320466900Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3fbc68a9c4554ff7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
