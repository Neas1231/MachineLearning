{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebb132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# In[106]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "with open('datasets/dataset.txt','r',encoding='utf-8') as file:\n",
    "    text_list = file.readlines()\n",
    "    data = pd.DataFrame(text_list)\n",
    "data\n",
    "\n",
    "\n",
    "# In[107]:\n",
    "\n",
    "\n",
    "def label_split(text):\n",
    "    separator = text.find(' ')\n",
    "    label = text[:separator]\n",
    "    comment = text[separator+1:]\n",
    "    return label, comment\n",
    "data = data[0].apply(label_split)\n",
    "\n",
    "\n",
    "# In[108]:\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data = list(data),columns=['label','comment'])\n",
    "df\n",
    "\n",
    "\n",
    "# In[109]:\n",
    "\n",
    "\n",
    "df.loc[df['label'] == '__label__NORMAL','label'] = 'normal'\n",
    "df.loc[df['label'] != 'normal','label'] = 'toxic'\n",
    "df\n",
    "\n",
    "\n",
    "# In[110]:\n",
    "\n",
    "\n",
    "df['label'].value_counts()\n",
    "\n",
    "\n",
    "# In[111]:\n",
    "\n",
    "\n",
    "data = pd.read_csv('datasets/cleaned_data_hatespeech.csv')\n",
    "data\n",
    "\n",
    "\n",
    "# In[112]:\n",
    "\n",
    "\n",
    "data = data[data['lang']=='ru']\n",
    "\n",
    "\n",
    "# In[113]:\n",
    "\n",
    "\n",
    "data.loc[data['toxic']==1,'toxic'] = 'toxic'\n",
    "data.loc[data['toxic']==0,'toxic'] = 'normal'\n",
    "\n",
    "\n",
    "# In[114]:\n",
    "\n",
    "\n",
    "data['toxic'].value_counts()\n",
    "\n",
    "\n",
    "# In[115]:\n",
    "\n",
    "\n",
    "data\n",
    "\n",
    "\n",
    "# In[116]:\n",
    "\n",
    "\n",
    "data.drop(columns='lang',inplace=True)\n",
    "data.columns = ['comment','label']\n",
    "\n",
    "\n",
    "# In[117]:\n",
    "\n",
    "\n",
    "df = pd.concat([df,data],axis=0)\n",
    "df\n",
    "\n",
    "\n",
    "# In[118]:\n",
    "\n",
    "\n",
    "data = pd.read_csv('datasets/labeled.csv')\n",
    "data\n",
    "\n",
    "\n",
    "# In[119]:\n",
    "\n",
    "\n",
    "data.loc[data['toxic']==1,'toxic'] = 'toxic'\n",
    "data.loc[data['toxic']==0,'toxic'] = 'normal'\n",
    "data.toxic.value_counts()\n",
    "\n",
    "\n",
    "# In[120]:\n",
    "\n",
    "\n",
    "data\n",
    "\n",
    "\n",
    "# In[121]:\n",
    "\n",
    "\n",
    "data.columns = ['comment','label']\n",
    "data\n",
    "\n",
    "\n",
    "# In[122]:\n",
    "\n",
    "\n",
    "df = pd.concat([df,data],axis=0)\n",
    "df\n",
    "\n",
    "\n",
    "# In[123]:\n",
    "\n",
    "\n",
    "data = pd.read_csv('datasets/labeled_1.csv')\n",
    "data\n",
    "\n",
    "\n",
    "# In[124]:\n",
    "\n",
    "\n",
    "data.loc[data['toxic']==1,'toxic'] = 'toxic'\n",
    "data.loc[data['toxic']==0,'toxic'] = 'normal'\n",
    "print(data.toxic.value_counts())\n",
    "data.columns = ['comment','label']\n",
    "data\n",
    "\n",
    "\n",
    "# In[125]:\n",
    "\n",
    "\n",
    "df = pd.concat([df,data],axis=0)\n",
    "df\n",
    "\n",
    "\n",
    "# In[126]:\n",
    "\n",
    "\n",
    "data = pd.read_csv('datasets/positive.csv')\n",
    "data\n",
    "\n",
    "\n",
    "# In[127]:\n",
    "\n",
    "\n",
    "data.drop(columns=['id','tdate','tname','trep','trtw','tfav','tstcount','tfoll','tfrien','listcount'],inplace=True)\n",
    "data\n",
    "\n",
    "\n",
    "# In[128]:\n",
    "\n",
    "\n",
    "data.columns = ['comment','label']\n",
    "data.loc[data['label'] == 1,'label'] = 'normal'\n",
    "data\n",
    "\n",
    "\n",
    "# In[129]:\n",
    "\n",
    "\n",
    "df = pd.concat([df,data],axis=0)\n",
    "df\n",
    "\n",
    "\n",
    "# In[130]:\n",
    "\n",
    "\n",
    "df.label.value_counts()\n",
    "\n",
    "\n",
    "# In[131]:\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "\n",
    "# In[132]:\n",
    "\n",
    "\n",
    "df_cop = df.copy()\n",
    "df_cop\n",
    "\n",
    "\n",
    "# In[133]:\n",
    "\n",
    "\n",
    "df_cop.isna().sum()\n",
    "\n",
    "\n",
    "# In[134]:\n",
    "\n",
    "\n",
    "df_cop.drop_duplicates(inplace=True)\n",
    "df_cop.reset_index(drop=True,inplace=True)\n",
    "df_cop\n",
    "\n",
    "\n",
    "# In[135]:\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "import re\n",
    "patterns = \"[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~â€”\\\"\\-]+\"\n",
    "stopwords_ru = stopwords.words(\"russian\")\n",
    "morph = MorphAnalyzer()\n",
    "stemmer = PorterStemmer()\n",
    "def lemmatize(text):\n",
    "    doc = re.sub(patterns, ' ', text)\n",
    "    tokens = word_tokenize(doc, language='russian')\n",
    "    lemmas = [morph.parse(token)[0].normal_form for token in tokens if token.lower() not in stopwords_ru]\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "df_cop['comment'] = df_cop['comment'].apply(lemmatize)\n",
    "\n",
    "\n",
    "# In[136]:\n",
    "\n",
    "\n",
    "def concat(text):\n",
    "    text= ' '.join(text)\n",
    "    return text\n",
    "\n",
    "df_cop['comment'] = df_cop['comment'].apply(concat)\n",
    "\n",
    "\n",
    "# In[137]:\n",
    "\n",
    "\n",
    "df_cop['comment']\n",
    "\n",
    "\n",
    "# In[138]:\n",
    "\n",
    "\n",
    "df_cop.to_csv('tokenized_data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
